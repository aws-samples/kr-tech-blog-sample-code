import streamlit as st
import boto3
import pandas as pd
from datetime import datetime, timedelta
import time
from typing import Dict, List
import logging
import os
from pathlib import Path

# Amazon Q Developer S3 ë¡œê·¸ ë¶„ì„ ëª¨ë“ˆ
from qcli_s3_analyzer import QCliS3LogAnalyzer


# ë¡œê¹… ì„¤ì •
def setup_logger():
    """ë””ë²„ê¹…ìš© ë¡œê±° ì„¤ì •"""
    log_dir = Path(__file__).parent / "log"
    log_dir.mkdir(exist_ok=True)

    log_filename = (
        log_dir / f"bedrock_tracker_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )

    logger = logging.getLogger("BedrockTracker")
    logger.setLevel(logging.DEBUG)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_filename, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
    )
    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.info(f"Logger initialized. Log file: {log_filename}")

    return logger


# ê¸€ë¡œë²Œ ë¡œê±°
logger = setup_logger()

# AWS Bedrock ëª¨ë¸ ê°€ê²© í…Œì´ë¸” (ë¦¬ì „ë³„)
# ì°¸ê³ : ìµœì‹  ê°€ê²©ì€ https://aws.amazon.com/bedrock/pricing/ ì—ì„œ í™•ì¸í•˜ì„¸ìš”
# ê°€ê²©ì€ USD ê¸°ì¤€ì´ë©°, 1000 í† í°ë‹¹ ê°€ê²©ì…ë‹ˆë‹¤
MODEL_PRICING = {
    # ê¸°ë³¸ ê°€ê²© (ëŒ€ë¶€ë¶„ì˜ ë¦¬ì „ì— ì ìš©)
    "default": {
        # Claude 3 ëª¨ë¸
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        # Claude 3.5 ëª¨ë¸
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        # Claude 3.7 ëª¨ë¸
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        # Claude 4 ëª¨ë¸
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US East (N. Virginia) - us-east-1
    "us-east-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US West (Oregon) - us-west-2
    "us-west-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Europe (Frankfurt) - eu-central-1
    "eu-central-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Tokyo) - ap-northeast-1
    "ap-northeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Seoul) - ap-northeast-2
    "ap-northeast-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Singapore) - ap-southeast-1
    "ap-southeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
}

# ë¦¬ì „ ì„¤ì •
REGIONS = {
    "us-east-1": "US East (N. Virginia)",
    "us-west-2": "US West (Oregon)",
    "eu-central-1": "Europe (Frankfurt)",
    "ap-northeast-1": "Asia Pacific (Tokyo)",
    "ap-northeast-2": "Asia Pacific (Seoul)",
    "ap-southeast-1": "Asia Pacific (Singapore)",
}

default_region = "us-east-1"


def get_model_cost(model_id: str, input_tokens: int, output_tokens: int, region: str = "default") -> float:
    """ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚° (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        model_id: Bedrock ëª¨ë¸ ID (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0)
        input_tokens: ì…ë ¥ í† í° ìˆ˜
        output_tokens: ì¶œë ¥ í† í° ìˆ˜
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        float: ê³„ì‚°ëœ ë¹„ìš© (USD)
    """
    logger.debug(
        f"Calculating cost for model: {model_id}, input: {input_tokens}, output: {output_tokens}, region: {region}"
    )

    # ëª¨ë¸ IDì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0 -> claude-3-haiku-20240307)
    model_name = model_id.split(".")[-1].split("-v")[0] if "." in model_id else model_id

    # ë¦¬ì „ë³„ ê°€ê²© í…Œì´ë¸” ì„ íƒ (í•´ë‹¹ ë¦¬ì „ì´ ì—†ìœ¼ë©´ default ì‚¬ìš©)
    region_pricing = MODEL_PRICING.get(region, MODEL_PRICING["default"])

    # ê°€ê²© í…Œì´ë¸”ì—ì„œ ëª¨ë¸ ì°¾ê¸°
    for key, pricing in region_pricing.items():
        if key in model_name:
            # ê°€ê²©ì€ 1000 í† í°ë‹¹ ê°€ê²©ì´ë¯€ë¡œ 1000ìœ¼ë¡œ ë‚˜ëˆ”
            cost = (input_tokens * pricing["input"] / 1000) + (
                output_tokens * pricing["output"] / 1000
            )
            logger.debug(f"Model: {key}, Region: {region}, Cost: ${cost:.6f}")
            return cost

    # ê¸°ë³¸ ê°€ê²© (Claude 3 Haiku)
    logger.warning(f"Unknown model: {model_id}, using default pricing (Claude 3 Haiku)")
    default_pricing = MODEL_PRICING["default"]["claude-3-haiku-20240307"]
    default_cost = (input_tokens * default_pricing["input"] / 1000) + (
        output_tokens * default_pricing["output"] / 1000
    )
    return default_cost


class BedrockAthenaTracker:
    def __init__(self, region=default_region):
        logger.info(f"Initializing BedrockAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        # STS í´ë¼ì´ì–¸íŠ¸ë„ regionì„ ì§€ì •í•˜ì—¬ ìƒì„±
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        # ë¦¬ì „ë³„ Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚·
        self.results_bucket = f"bedrock-analytics-{self.account_id}-{self.region}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def get_current_logging_config(self) -> Dict:
        """í˜„ì¬ ì„¤ì •ëœ Model Invocation Logging ì •ë³´ ì¡°íšŒ"""
        logger.info("Getting current logging configuration")
        try:
            bedrock = boto3.client("bedrock", region_name=self.region)
            response = bedrock.get_model_invocation_logging_configuration()

            if "loggingConfig" in response:
                config = response["loggingConfig"]

                if "s3Config" in config:
                    result = {
                        "type": "s3",
                        "bucket": config["s3Config"].get("bucketName", ""),
                        "prefix": config["s3Config"].get("keyPrefix", ""),
                        "status": "enabled",
                    }
                    logger.info(f"Logging config: {result}")
                    return result

            logger.warning("Logging is disabled")
            return {"status": "disabled"}

        except Exception as e:
            logger.error(f"Error getting logging config: {str(e)}")
            return {"status": "error", "error": str(e)}

    def set_results_bucket(self, bucket_name: str):
        """Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚· ì„¤ì •"""
        self.results_bucket = bucket_name
        logger.info(f"Results bucket set to: {self.results_bucket}")

    def execute_athena_query(
        self, query: str, database: str = "bedrock_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            st.error(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def get_user_cost_analysis(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ë¹„ìš© ë¶„ì„"""
        logger.info(f"Getting user cost analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_user_app_detail_analysis(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„"""
        logger.info(f"Getting user-app detail analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn, modelId
        ORDER BY user_or_app, call_count DESC
        """

        return self.execute_athena_query(query)

    def get_hourly_usage_pattern(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì‹œê°„ë³„ ì‚¬ìš© íŒ¨í„´ - timestampì—ì„œ hour ì¶”ì¶œ"""
        logger.info(f"Getting hourly usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year,
            month,
            day,
            date_format(from_iso8601_timestamp(timestamp), '%H') as hour,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        ORDER BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(f"Getting daily usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year, month, day,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day
        ORDER BY year, month, day
        """

        return self.execute_athena_query(query)

    def get_model_usage_stats(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„"""
        logger.info(f"Getting model usage stats from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            AVG(CAST(input.inputTokenCount AS DOUBLE)) as avg_input_tokens,
            AVG(CAST(output.outputTokenCount AS DOUBLE)) as avg_output_tokens,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY modelId
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_total_summary(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„"""
        logger.info(f"Getting total summary from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            COUNT(*) as total_calls,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty:
            result = {
                "total_calls": (
                    int(df.iloc[0]["total_calls"]) if df.iloc[0]["total_calls"] else 0
                ),
                "total_input_tokens": (
                    int(df.iloc[0]["total_input_tokens"])
                    if df.iloc[0]["total_input_tokens"]
                    else 0
                ),
                "total_output_tokens": (
                    int(df.iloc[0]["total_output_tokens"])
                    if df.iloc[0]["total_output_tokens"]
                    else 0
                ),
                "total_cost_usd": 0.0,  # ëª¨ë¸ë³„ë¡œ ê³„ì‚° í•„ìš”
            }
            logger.info(f"Total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "total_calls": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost_usd": 0.0,
            }


# QCli í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • ìƒìˆ˜
# ê¸°ì¤€: ì˜ì–´ ë‹¨ì–´ 1.4í† í°, 4ê¸€ìë‹¹ 5í† í°
# ì½”ë“œ 1ì¤„ í‰ê·  60-80ë¬¸ì = ì•½ 75-100í† í°
QCLI_TOKEN_ESTIMATION = {
    "conservative": {  # ë³´ìˆ˜ì  ì¶”ì • (ì§§ì€ ì½”ë“œ/ê°„ë‹¨í•œ ì§ˆë¬¸)
        "chat_message_input": 100,
        "chat_message_output": 300,
        "chat_code_line": 50,
        "inline_suggestion": 40,
        "inline_code_line": 50,
        "dev_event_input": 400,
        "dev_event_output": 600,
        "test_event_input": 300,
        "test_event_output": 500,
        "doc_event_input": 200,
        "doc_event_output": 400,
    },
    "average": {  # í‰ê·  ì¶”ì • (ì¼ë°˜ì ì¸ ì‚¬ìš©) - ê¶Œì¥
        "chat_message_input": 150,
        "chat_message_output": 500,
        "chat_code_line": 75,
        "inline_suggestion": 60,
        "inline_code_line": 75,
        "dev_event_input": 600,
        "dev_event_output": 1000,
        "test_event_input": 450,
        "test_event_output": 750,
        "doc_event_input": 350,
        "doc_event_output": 600,
    },
    "optimistic": {  # ë‚™ê´€ì  ì¶”ì • (ë³µì¡í•œ ì½”ë“œ/ê¸´ ëŒ€í™”)
        "chat_message_input": 200,
        "chat_message_output": 800,
        "chat_code_line": 100,
        "inline_suggestion": 80,
        "inline_code_line": 100,
        "dev_event_input": 1000,
        "dev_event_output": 1500,
        "test_event_input": 600,
        "test_event_output": 1000,
        "doc_event_input": 500,
        "doc_event_output": 800,
    }
}


class QCliAthenaTracker:
    """Amazon Q CLI ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ Athena ì¿¼ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, region=default_region):
        logger.info(f"Initializing QCliAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        self.results_bucket = f"amazonq-developer-reports-{self.account_id}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def execute_athena_query(
        self, query: str, database: str = "qcli_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            st.error(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def get_total_summary(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„ - Amazon Q Developer CSV ë¦¬í¬íŠ¸ ê¸°ë°˜"""
        logger.info(
            f"Getting QCli total summary from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        # ì‹¤ì œ AWS CSV ë©”íŠ¸ë¦­ ì‚¬ìš©:
        # - Chat_MessagesSent: ì±„íŒ… ë©”ì‹œì§€ ìˆ˜
        # - Inline_SuggestionsCount: ì¸ë¼ì¸ ì½”ë“œ ì œì•ˆ ìˆ˜
        # - Chat_MessagesInteracted: ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë©”íŠ¸ë¦­
        query = f"""
        SELECT
            COUNT(DISTINCT UserId) as unique_users,
            COUNT(DISTINCT Date) as active_days,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_dev_events,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_test_events
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty and df.iloc[0]["unique_users"]:
            result = {
                "unique_users": (
                    int(df.iloc[0]["unique_users"])
                    if df.iloc[0]["unique_users"]
                    else 0
                ),
                "active_days": (
                    int(df.iloc[0]["active_days"]) if df.iloc[0]["active_days"] else 0
                ),
                "total_chat_messages": (
                    int(df.iloc[0]["total_chat_messages"])
                    if df.iloc[0]["total_chat_messages"]
                    else 0
                ),
                "total_inline_suggestions": (
                    int(df.iloc[0]["total_inline_suggestions"])
                    if df.iloc[0]["total_inline_suggestions"]
                    else 0
                ),
                "total_inline_acceptances": (
                    int(df.iloc[0]["total_inline_acceptances"])
                    if df.iloc[0]["total_inline_acceptances"]
                    else 0
                ),
                "total_chat_code_lines": (
                    int(df.iloc[0]["total_chat_code_lines"])
                    if df.iloc[0]["total_chat_code_lines"]
                    else 0
                ),
                "total_inline_code_lines": (
                    int(df.iloc[0]["total_inline_code_lines"])
                    if df.iloc[0]["total_inline_code_lines"]
                    else 0
                ),
                "total_dev_events": (
                    int(df.iloc[0]["total_dev_events"])
                    if df.iloc[0]["total_dev_events"]
                    else 0
                ),
                "total_test_events": (
                    int(df.iloc[0]["total_test_events"])
                    if df.iloc[0]["total_test_events"]
                    else 0
                ),
            }
            logger.info(f"QCli total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "unique_users": 0,
                "active_days": 0,
                "total_chat_messages": 0,
                "total_inline_suggestions": 0,
                "total_inline_acceptances": 0,
                "total_chat_code_lines": 0,
                "total_inline_code_lines": 0,
                "total_dev_events": 0,
                "total_test_events": 0,
            }

    def get_user_usage_analysis(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        logger.info(
            f"Getting QCli user usage analysis from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            UserId as user_id,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_dev_events,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_test_events,
            SUM(CAST(DocGeneration_EventCount AS BIGINT)) as total_doc_events,
            COUNT(DISTINCT Date) as active_days,
            MIN(Date) as first_activity,
            MAX(Date) as last_activity
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        GROUP BY UserId
        ORDER BY total_chat_messages DESC
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(
            f"Getting QCli daily usage pattern from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            Date as date_str,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        GROUP BY Date
        ORDER BY Date
        """

        return self.execute_athena_query(query)


    def get_feature_usage_stats(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„ (Chat, Inline, Dev, Test, Doc ë“±)"""
        logger.info(
            f"Getting QCli feature usage stats from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            'Chat Messages' as feature_type,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            'Inline Suggestions' as feature_type,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            'Inline Acceptances' as feature_type,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/dev Events' as feature_type,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/test Events' as feature_type,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/doc Events' as feature_type,
            SUM(CAST(DocGeneration_EventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        ORDER BY total_count DESC
        """

        return self.execute_athena_query(query)

    def estimate_tokens(self, summary: Dict, estimation_type: str = "average") -> Dict:
        """ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¡œë¶€í„° í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •

        Args:
            summary: get_total_summary()ì—ì„œ ë°˜í™˜ëœ ìš”ì•½ ë°ì´í„°
            estimation_type: "conservative", "average", "optimistic" ì¤‘ ì„ íƒ

        Returns:
            Dict: ì¶”ì •ëœ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
        """
        logger.info(f"Estimating tokens with {estimation_type} model")

        if estimation_type not in QCLI_TOKEN_ESTIMATION:
            logger.warning(f"Unknown estimation type: {estimation_type}, using 'average'")
            estimation_type = "average"

        constants = QCLI_TOKEN_ESTIMATION[estimation_type]

        # Input í† í° ì¶”ì •
        estimated_input_tokens = (
            summary.get("total_chat_messages", 0) * constants["chat_message_input"] +
            summary.get("total_inline_suggestions", 0) * constants["inline_suggestion"] +
            summary.get("total_dev_events", 0) * constants["dev_event_input"] +
            summary.get("total_test_events", 0) * constants["test_event_input"] +
            (summary.get("total_doc_events", 0) if "total_doc_events" in summary else 0) * constants["doc_event_input"]
        )

        # Output í† í° ì¶”ì •
        estimated_output_tokens = (
            summary.get("total_chat_messages", 0) * constants["chat_message_output"] +
            summary.get("total_chat_code_lines", 0) * constants["chat_code_line"] +
            summary.get("total_inline_code_lines", 0) * constants["inline_code_line"] +
            summary.get("total_inline_acceptances", 0) * constants["inline_suggestion"] +
            summary.get("total_dev_events", 0) * constants["dev_event_output"] +
            summary.get("total_test_events", 0) * constants["test_event_output"] +
            (summary.get("total_doc_events", 0) if "total_doc_events" in summary else 0) * constants["doc_event_output"]
        )

        total_tokens = estimated_input_tokens + estimated_output_tokens

        result = {
            "estimation_type": estimation_type,
            "estimated_input_tokens": int(estimated_input_tokens),
            "estimated_output_tokens": int(estimated_output_tokens),
            "estimated_total_tokens": int(total_tokens),
        }

        logger.info(f"Token estimation result: {result}")
        return result

    def check_official_limits(self, summary: Dict, days_in_period: int) -> Dict:
        """ê³µì‹ ë¦¬ë°‹ ì²´í¬ ë° ê²½ê³  ìƒì„±

        Args:
            summary: get_total_summary()ì—ì„œ ë°˜í™˜ëœ ìš”ì•½ ë°ì´í„°
            days_in_period: ì¡°íšŒ ê¸°ê°„ì˜ ì¼ìˆ˜

        Returns:
            Dict: ë¦¬ë°‹ ì²´í¬ ê²°ê³¼ ë° ê²½ê³ 
        """
        logger.info("Checking official limits")

        # ê³µì‹ ë¬¸ì„œí™”ëœ ë¦¬ë°‹
        OFFICIAL_LIMITS = {
            "dev_events": 30,  # /dev ëª…ë ¹ì–´: 30íšŒ/ì›”
            "transformation_lines": 4000,  # Code Transformation: 4,000ì¤„/ì›”
            # ì±„íŒ…/ì¸ë¼ì¸: AWSê°€ ê³µê°œí•˜ì§€ ì•ŠìŒ
        }

        # ì›”ê°„ ì‚¬ìš©ëŸ‰ ì¶”ì • (í˜„ì¬ ê¸°ê°„ì„ 30ì¼ë¡œ í™˜ì‚°)
        monthly_factor = 30 / days_in_period if days_in_period > 0 else 1

        dev_events_used = summary.get("total_dev_events", 0)
        dev_events_projected = int(dev_events_used * monthly_factor)

        # Transformation ë°ì´í„°ëŠ” summaryì— ì—†ì„ ìˆ˜ ìˆìŒ (ì¶”í›„ ì¶”ê°€ ê°€ëŠ¥)
        transformation_lines_used = summary.get("total_transformation_lines", 0)
        transformation_lines_projected = int(transformation_lines_used * monthly_factor)

        result = {
            "dev_events": {
                "used": dev_events_used,
                "limit": OFFICIAL_LIMITS["dev_events"],
                "projected_monthly": dev_events_projected,
                "percentage": (dev_events_projected / OFFICIAL_LIMITS["dev_events"] * 100) if OFFICIAL_LIMITS["dev_events"] > 0 else 0,
                "warning": dev_events_projected >= OFFICIAL_LIMITS["dev_events"] * 0.8
            },
            "transformation_lines": {
                "used": transformation_lines_used,
                "limit": OFFICIAL_LIMITS["transformation_lines"],
                "projected_monthly": transformation_lines_projected,
                "percentage": (transformation_lines_projected / OFFICIAL_LIMITS["transformation_lines"] * 100) if OFFICIAL_LIMITS["transformation_lines"] > 0 else 0,
                "warning": transformation_lines_projected >= OFFICIAL_LIMITS["transformation_lines"] * 0.8
            }
        }

        logger.info(f"Limit check result: {result}")
        return result

    def analyze_usage_trends(self, start_date: datetime, end_date: datetime, user_pattern: str = None) -> Dict:
        """ì‚¬ìš©ëŸ‰ ì¶”ì„¸ ë¶„ì„ ë° ì´ìƒ ê°ì§€

        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            user_pattern: ì‚¬ìš©ì í•„í„° íŒ¨í„´

        Returns:
            Dict: ì¶”ì„¸ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"Analyzing usage trends from {start_date} to {end_date}")

        # ì¼ë³„ ì‚¬ìš© íŒ¨í„´ ì¡°íšŒ
        daily_df = self.get_daily_usage_pattern(start_date, end_date, user_pattern)

        if daily_df.empty:
            return {
                "daily_avg": 0,
                "daily_max": 0,
                "daily_min": 0,
                "anomaly_detected": False
            }

        # ìˆ«ì ë³€í™˜
        for col in ["total_chat_messages", "total_inline_suggestions"]:
            if col in daily_df.columns:
                daily_df[col] = pd.to_numeric(daily_df[col], errors='coerce').fillna(0)

        # ì¼ì¼ ì´ í™œë™ ê³„ì‚°
        daily_df["total_activity"] = (
            daily_df.get("total_chat_messages", 0) +
            daily_df.get("total_inline_suggestions", 0)
        )

        daily_avg = daily_df["total_activity"].mean()
        daily_max = daily_df["total_activity"].max()
        daily_min = daily_df["total_activity"].min()

        # ì´ìƒ ê°ì§€: ì¼í‰ê· ì˜ 3ë°° ì´ˆê³¼í•˜ëŠ” ë‚ ì´ ìˆëŠ”ì§€
        anomaly_threshold = daily_avg * 3
        anomaly_days = daily_df[daily_df["total_activity"] > anomaly_threshold]

        result = {
            "daily_avg": float(daily_avg),
            "daily_max": float(daily_max),
            "daily_min": float(daily_min),
            "anomaly_detected": len(anomaly_days) > 0,
            "anomaly_count": len(anomaly_days),
            "anomaly_threshold": float(anomaly_threshold)
        }

        logger.info(f"Trend analysis result: {result}")
        return result


def calculate_cost_for_dataframe(
    df: pd.DataFrame, model_col: str = "model_name", region: str = "default"
) -> pd.DataFrame:
    """DataFrameì— ë¹„ìš© ì»¬ëŸ¼ ì¶”ê°€ (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        df: ë¹„ìš©ì„ ê³„ì‚°í•  DataFrame
        model_col: ëª¨ë¸ëª…ì´ ìˆëŠ” ì»¬ëŸ¼ëª…
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        pd.DataFrame: ë¹„ìš© ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    logger.info(f"Calculating cost for DataFrame with {len(df)} rows, region: {region}")

    if df.empty:
        return df

    costs = []
    for _, row in df.iterrows():
        model = row.get(model_col, "")
        input_tokens = (
            int(row.get("total_input_tokens", 0))
            if row.get("total_input_tokens")
            else 0
        )
        output_tokens = (
            int(row.get("total_output_tokens", 0))
            if row.get("total_output_tokens")
            else 0
        )
        cost = get_model_cost(model, input_tokens, output_tokens, region)
        costs.append(cost)

    df["estimated_cost_usd"] = costs
    logger.info(f"Total cost calculated for region {region}: ${sum(costs):.4f}")
    return df


def main():
    logger.info("Starting Analytics Dashboard")

    st.set_page_config(
        page_title="AWS Analytics Dashboard", page_icon="ğŸ“Š", layout="wide"
    )

    st.title("ğŸ“Š AWS Analytics Dashboard")
    st.markdown("**Athena ê¸°ë°˜ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ ë¶„ì„ - Bedrock & Amazon Q CLI**")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

    # ë¶„ì„ ìœ í˜• ì„ íƒ
    analysis_type = st.sidebar.radio(
        "ë¶„ì„ ìœ í˜• ì„ íƒ",
        ["AWS Bedrock", "Amazon Q CLI"],
        index=0
    )

    # ë¦¬ì „ ì„ íƒ
    if analysis_type == "Amazon Q CLI":
        # Amazon Q CLIëŠ” us-east-1ì—ì„œë§Œ ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ ê´€ë¦¬
        st.sidebar.info("â„¹ï¸ Amazon Q CLI ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ëŠ” us-east-1ì—ì„œë§Œ ê´€ë¦¬ë©ë‹ˆë‹¤.")
        selected_region = "us-east-1"
        st.sidebar.text(f"ë¦¬ì „: {selected_region} - {REGIONS[selected_region]} (ê³ ì •)")
    else:
        # Bedrockì€ ëª¨ë“  ë¦¬ì „ ì„ íƒ ê°€ëŠ¥
        selected_region = st.sidebar.selectbox(
            "ë¦¬ì „ ì„ íƒ",
            options=list(REGIONS.keys()),
            format_func=lambda x: f"{x} - {REGIONS[x]}",
            index=4,
        )

    logger.info(f"Selected region: {selected_region}, Analysis type: {analysis_type}")

    # ë‚ ì§œ ë²”ìœ„ ì„ íƒ
    st.sidebar.subheader("ğŸ“… ë‚ ì§œ ë²”ìœ„ ì„ íƒ")

    col1, col2 = st.sidebar.columns(2)
    with col1:
        start_date = st.date_input(
            "ì‹œì‘ ë‚ ì§œ",
            value=datetime.now() - timedelta(days=7),
            max_value=datetime.now(),
        )
    with col2:
        end_date = st.date_input(
            "ì¢…ë£Œ ë‚ ì§œ", value=datetime.now(), max_value=datetime.now()
        )

    logger.info(f"Date range: {start_date} to {end_date}")

    # ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ëŒ€ì‹œë³´ë“œ ë Œë”ë§
    if analysis_type == "AWS Bedrock":
        render_bedrock_analytics(selected_region, start_date, end_date)
    else:
        render_qcli_analytics(selected_region, start_date, end_date)


def render_bedrock_analytics(selected_region, start_date, end_date):
    """Bedrock ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    logger.info("Rendering Bedrock Analytics")

    # ARN íŒ¨í„´ í•„í„°
    st.sidebar.subheader("ğŸ” ARN íŒ¨í„´ í•„í„° (ì„ íƒì‚¬í•­)")
    arn_pattern = st.sidebar.text_input(
        "ARN íŒ¨í„´",
        value="",
        placeholder="ì˜ˆ: AmazonQ-CLI, q-cli",
        key="bedrock_arn_pattern",
        help="íŠ¹ì • ARN íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ì‚¬ìš©ìë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ì‚¬ìš©ìë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
    )

    # í˜„ì¬ ë¡œê¹… ì„¤ì • ìë™ ì¡°íšŒ
    tracker = BedrockAthenaTracker(region=selected_region)

    with st.spinner("í˜„ì¬ Model Invocation Logging ì„¤ì • í™•ì¸ ì¤‘..."):
        current_config = tracker.get_current_logging_config()

    # ì„¤ì • ìƒíƒœ í‘œì‹œ
    if current_config["status"] == "enabled":
        st.success("âœ… Model Invocation Loggingì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")

        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ“ **S3 ë²„í‚·**: ì„¤ì •ë¨ ({selected_region})")
        with col2:
            st.info(f"ğŸ“‚ **í”„ë¦¬í”½ìŠ¤**: ì„¤ì •ë¨")

    elif current_config["status"] == "disabled":
        st.error("âŒ Model Invocation Loggingì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        st.markdown("ğŸ‘‡ ë¨¼ì € ì„¤ì •ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”:")
        st.code("python setup_bedrock_analytics.py")
        logger.error("Model Invocation Logging is disabled")
        return

    else:
        st.warning(
            f"âš ï¸ ì„¤ì • í™•ì¸ ì¤‘ ì˜¤ë¥˜: {current_config.get('error', 'Unknown error')}"
        )
        logger.error(f"Error checking logging config: {current_config.get('error')}")
        return

    # ë¶„ì„ ì‹¤í–‰
    if st.sidebar.button("ğŸ” ë°ì´í„° ë¶„ì„", type="primary"):
        logger.info("Analysis button clicked")

        with st.spinner("Athenaì—ì„œ ë°ì´í„° ë¶„ì„ ì¤‘..."):

            # ARN íŒ¨í„´ ì •ë³´ í‘œì‹œ
            if arn_pattern:
                st.info(f"ğŸ” ARN íŒ¨í„´ í•„í„°ë§ ì ìš©: '{arn_pattern}'")

            # ì „ì²´ ìš”ì•½
            summary = tracker.get_total_summary(start_date, end_date, arn_pattern if arn_pattern else None)

            st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ì´ API í˜¸ì¶œ", f"{summary['total_calls']:,}")

            with col2:
                st.metric("ì´ Input í† í°", f"{summary['total_input_tokens']:,}")

            with col3:
                st.metric("ì´ Output í† í°", f"{summary['total_output_tokens']:,}")

            # ëª¨ë¸ë³„ í†µê³„ë¡œ ì´ ë¹„ìš© ê³„ì‚°
            model_df = tracker.get_model_usage_stats(start_date, end_date, arn_pattern if arn_pattern else None)
            if not model_df.empty:
                model_df = calculate_cost_for_dataframe(model_df, region=selected_region)
                total_cost = model_df["estimated_cost_usd"].sum()
                summary["total_cost_usd"] = total_cost

            with col4:
                st.metric("ì´ ë¹„ìš©", f"${summary['total_cost_usd']:.4f}")

            # ì‚¬ìš©ìë³„ ë¶„ì„
            st.header("ğŸ‘¥ ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¶„ì„")

            user_df = tracker.get_user_cost_analysis(start_date, end_date, arn_pattern if arn_pattern else None)

            if not user_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(
                            user_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚°ì„ ìœ„í•œ ì„ì‹œ ëª¨ë¸ëª… ì¶”ê°€ (ëª¨ë¸ë³„ í‰ê·  ì‚¬ìš©)
                # ì‹¤ì œë¡œëŠ” ê° ì‚¬ìš©ìê°€ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ì•Œì•„ì•¼ ì •í™•í•¨
                # ì—¬ê¸°ì„œëŠ” Claude 3 Haiku ê¸°ë³¸ ê°€ê²© ì‚¬ìš© (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)
                costs = []
                for _, row in user_df.iterrows():
                    input_tokens = int(row.get("total_input_tokens", 0)) if row.get("total_input_tokens") else 0
                    output_tokens = int(row.get("total_output_tokens", 0)) if row.get("total_output_tokens") else 0
                    # Claude 3 Haikuë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©
                    cost = get_model_cost("claude-3-haiku-20240307", input_tokens, output_tokens, selected_region)
                    costs.append(cost)
                user_df["estimated_cost_usd"] = costs

                st.dataframe(user_df, use_container_width=True)

                # ë¹„ìš© ì°¨íŠ¸
                if len(user_df) > 0:
                    import plotly.express as px

                    fig = px.bar(
                        user_df.head(10),
                        x="user_or_app",
                        y="estimated_cost_usd",
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¹„ìš©",
                        labels={
                            "user_or_app": "ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜",
                            "estimated_cost_usd": "ë¹„ìš© (USD)",
                        },
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„
            st.header("ğŸ“± ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„")

            user_app_df = tracker.get_user_app_detail_analysis(start_date, end_date, arn_pattern if arn_pattern else None)

            if not user_app_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_app_df.columns:
                        user_app_df[col] = pd.to_numeric(
                            user_app_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚° (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)
                user_app_df = calculate_cost_for_dataframe(user_app_df, region=selected_region)

                st.dataframe(user_app_df, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ëª¨ë¸ë³„ ë¶„ì„
            st.header("ğŸ¤– ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„")

            if not model_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "avg_input_tokens",
                    "avg_output_tokens",
                    "total_input_tokens",
                    "total_output_tokens",
                    "estimated_cost_usd",
                ]
                for col in numeric_columns:
                    if col in model_df.columns:
                        model_df[col] = pd.to_numeric(
                            model_df[col], errors="coerce"
                        ).fillna(0)

                st.dataframe(model_df, use_container_width=True)

                # ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨ ì°¨íŠ¸
                if len(model_df) > 0:
                    import plotly.express as px

                    fig = px.pie(
                        model_df,
                        values="call_count",
                        names="model_name",
                        title="ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨",
                    )
                    st.plotly_chart(fig, use_container_width=True)

            # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
            st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

            daily_df = tracker.get_daily_usage_pattern(start_date, end_date, arn_pattern if arn_pattern else None)

            if not daily_df.empty and len(daily_df) > 0:
                # ë‚ ì§œ ì»¬ëŸ¼ ìƒì„± (ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ zfill ì ìš©)
                daily_df["date"] = pd.to_datetime(
                    daily_df["year"].astype(str)
                    + "-"
                    + daily_df["month"].astype(str).str.zfill(2)
                    + "-"
                    + daily_df["day"].astype(str).str.zfill(2)
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = ["call_count", "total_input_tokens", "total_output_tokens"]
                for col in numeric_columns:
                    if col in daily_df.columns:
                        daily_df[col] = pd.to_numeric(
                            daily_df[col], errors="coerce"
                        ).fillna(0)

                # í‘œì‹œìš© DataFrame ìƒì„± (ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·)
                display_df = daily_df.copy()
                display_df["ë‚ ì§œ"] = display_df["date"].dt.strftime("%Y-%m-%d")
                display_df = display_df[["ë‚ ì§œ", "call_count", "total_input_tokens", "total_output_tokens"]]
                display_df.columns = ["ë‚ ì§œ", "API í˜¸ì¶œ ìˆ˜", "Input í† í°", "Output í† í°"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì¼ë³„ API í˜¸ì¶œ íŒ¨í„´
                fig = px.line(
                    daily_df,
                    x="date",
                    y="call_count",
                    title="ì¼ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"date": "ë‚ ì§œ", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                    markers=True
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰
                fig2 = go.Figure()
                fig2.add_trace(go.Scatter(
                    x=daily_df["date"],
                    y=daily_df["total_input_tokens"],
                    mode='lines+markers',
                    name='Input í† í°',
                    line=dict(color='blue')
                ))
                fig2.add_trace(go.Scatter(
                    x=daily_df["date"],
                    y=daily_df["total_output_tokens"],
                    mode='lines+markers',
                    name='Output í† í°',
                    line=dict(color='red')
                ))
                fig2.update_layout(
                    title="ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰",
                    xaxis_title="ë‚ ì§œ",
                    yaxis_title="í† í° ìˆ˜",
                    hovermode='x unified'
                )
                st.plotly_chart(fig2, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì¼ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            st.header("â° ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´")

            hourly_df = tracker.get_hourly_usage_pattern(start_date, end_date, arn_pattern if arn_pattern else None)

            if not hourly_df.empty and len(hourly_df) > 0:
                # ì‹œê°„ ì»¬ëŸ¼ ìƒì„± (ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ zfill ì ìš©)
                hourly_df["datetime"] = pd.to_datetime(
                    hourly_df["year"].astype(str)
                    + "-"
                    + hourly_df["month"].astype(str).str.zfill(2)
                    + "-"
                    + hourly_df["day"].astype(str).str.zfill(2)
                    + " "
                    + hourly_df["hour"].astype(str).str.zfill(2)
                    + ":00:00"
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = ["call_count", "total_input_tokens", "total_output_tokens"]
                for col in numeric_columns:
                    if col in hourly_df.columns:
                        hourly_df[col] = pd.to_numeric(
                            hourly_df[col], errors="coerce"
                        ).fillna(0)

                # í‘œì‹œìš© DataFrame ìƒì„±
                display_df = hourly_df.copy()
                display_df["ì‹œê°„"] = display_df["datetime"].dt.strftime("%Y-%m-%d %H:00")
                display_df = display_df[["ì‹œê°„", "call_count", "total_input_tokens", "total_output_tokens"]]
                display_df.columns = ["ì‹œê°„", "API í˜¸ì¶œ ìˆ˜", "Input í† í°", "Output í† í°"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì‹œê°„ëŒ€ë³„ API í˜¸ì¶œ íŒ¨í„´
                fig = px.line(
                    hourly_df,
                    x="datetime",
                    y="call_count",
                    title="ì‹œê°„ëŒ€ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"datetime": "ì‹œê°„", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                    markers=True
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì‹œê°„ëŒ€ë³„ í† í° ì‚¬ìš©ëŸ‰
                fig2 = go.Figure()
                fig2.add_trace(go.Scatter(
                    x=hourly_df["datetime"],
                    y=hourly_df["total_input_tokens"],
                    mode='lines+markers',
                    name='Input í† í°',
                    line=dict(color='blue')
                ))
                fig2.add_trace(go.Scatter(
                    x=hourly_df["datetime"],
                    y=hourly_df["total_output_tokens"],
                    mode='lines+markers',
                    name='Output í† í°',
                    line=dict(color='red')
                ))
                fig2.update_layout(
                    title="ì‹œê°„ëŒ€ë³„ í† í° ì‚¬ìš©ëŸ‰",
                    xaxis_title="ì‹œê°„",
                    yaxis_title="í† í° ìˆ˜",
                    hovermode='x unified'
                )
                st.plotly_chart(fig2, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì‹œê°„ëŒ€ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        # ì´ˆê¸° í™”ë©´
        st.info(
            "ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¦¬ì „ê³¼ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•œ í›„ 'ë°ì´í„° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
        )

        st.markdown("### ğŸ› ï¸ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ")

        st.markdown("#### 1ï¸âƒ£ í™˜ê²½ ìš”êµ¬ì‚¬í•­")
        st.markdown(
            """
        **AWS ê¶Œí•œ**: ë‹¤ìŒ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤
        - Bedrock: InvokeModel, Get/PutModelInvocationLoggingConfiguration
        - S3: GetObject, ListBucket, PutObject, CreateBucket
        - Athena: StartQueryExecution, GetQueryExecution, GetQueryResults
        - Glue: CreateDatabase, CreateTable, GetDatabase, GetTable

        **Python í™˜ê²½**:
        - Python 3.8 ì´ìƒ
        - boto3, streamlit, pandas, plotly
        """
        )

        st.markdown("#### 2ï¸âƒ£ ì„¤ì¹˜ ë°©ë²•")
        st.code(
            """
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 2. AWS ìê²©ì¦ëª… ì„¤ì •
aws configure
# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=us-east-1
        """,
            language="bash"
        )

        st.markdown("#### 3ï¸âƒ£ ì´ˆê¸° ì„¤ì • ë‹¨ê³„")
        st.code(
            """
# Step 1: Athena ë¶„ì„ í™˜ê²½ êµ¬ì¶•
python setup_athena_bucket.py

# Step 2: Bedrock ë¡œê¹… ì„¤ì • í™•ì¸ ë° í™œì„±í™”
python check_bedrock_logging.py
python setup_bedrock_logging.py

# Step 3: IAM Role ê¶Œí•œ ê²€ì¦
python verify_bedrock_permissions.py

# Step 4: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì„ íƒì‚¬í•­)
python generate_test_data.py

# Step 5: ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
streamlit run bedrock_tracker.py
        """,
            language="bash"
        )

        st.markdown("### ğŸ“‹ ì§€ì› ëª¨ë¸")
        st.markdown(
            """
        - **Claude 3**: Haiku, Sonnet, Opus
        - **Claude 3.5**: Haiku, Sonnet
        - **Claude 3.7**: Sonnet
        - **Claude 4**: Sonnet 4, Sonnet 4.5
        - **Claude 4.1**: Opus
        """
        )

        st.markdown("### ğŸŒ ì§€ì› ë¦¬ì „")
        for region_id, region_name in REGIONS.items():
            st.markdown(f"- **{region_id}**: {region_name}")

    logger.info("Bedrock Dashboard rendering complete")


def render_qcli_analytics(selected_region, start_date, end_date):
    """Amazon Q CLI ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    logger.info("Rendering Amazon Q CLI Analytics")

    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
    st.sidebar.subheader("ğŸ“Š ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ")
    data_source = st.sidebar.radio(
        "ë¶„ì„ ë°ì´í„° ì†ŒìŠ¤",
        options=["S3 ë¡œê·¸ (ì‹¤ì œ í† í°)", "Athena CSV (ì¶”ì •)"],
        index=0,
        key="qcli_data_source",
        help="S3 ë¡œê·¸: ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ì—ì„œ í† í° ê³„ì‚° (ì •í™•)\nAthena CSV: ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ì—ì„œ í† í° ì¶”ì • (ë¹ ë¦„)"
    )

    # ì‚¬ìš©ì íŒ¨í„´ í•„í„°
    st.sidebar.subheader("ğŸ” ì‚¬ìš©ì ID í•„í„° (ì„ íƒì‚¬í•­)")
    user_pattern = st.sidebar.text_input(
        "ì‚¬ìš©ì ID íŒ¨í„´",
        value="",
        placeholder="ì˜ˆ: user@example.com",
        key="qcli_user_pattern",
        help="íŠ¹ì • ì‚¬ìš©ì ID íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ì‚¬ìš©ìë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ì‚¬ìš©ìë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
    )

    # ë°ì´í„° ì†ŒìŠ¤ì— ë”°ë¼ ë‹¤ë¥¸ ì •ë³´ í‘œì‹œ
    if data_source == "S3 ë¡œê·¸ (ì‹¤ì œ í† í°)":
        st.info(
            "ğŸ“‹ **Amazon Q Developer S3 ë¡œê·¸ ë¶„ì„** (ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰)\n\n"
            "ì´ ë¶„ì„ì€ S3ì— ì €ì¥ëœ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ë¥¼ ì½ì–´ tiktokenìœ¼ë¡œ í† í°ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
            "IDEì—ì„œ ë°œìƒí•œ Chat ë° Inline ì œì•ˆì˜ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    else:
        st.info(
            "ğŸ“‹ **Amazon Q Developer Athena ë¶„ì„** (í† í° ì¶”ì •)\n\n"
            "ì´ ëŒ€ì‹œë³´ë“œëŠ” Amazon Q Developerì˜ ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ CSV íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n"
            "CSV ë¦¬í¬íŠ¸ëŠ” ë§¤ì¼ ìì •(UTC)ì— ìƒì„±ë˜ë©° S3 ë²„í‚·ì— ì €ì¥ë©ë‹ˆë‹¤."
        )

    # ë¶„ì„ ì‹¤í–‰
    if st.sidebar.button("ğŸ” ë°ì´í„° ë¶„ì„", type="primary", key="qcli_analyze"):
        logger.info("QCli Analysis button clicked")

        # ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ë¶„ì„ ë¡œì§ ì‹¤í–‰
        if data_source == "S3 ë¡œê·¸ (ì‹¤ì œ í† í°)":
            # ===== S3 ë¡œê·¸ ë¶„ì„ =====
            with st.spinner("S3ì—ì„œ Amazon Q Developer í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ë¶„ì„ ì¤‘..."):

                # ì‚¬ìš©ì íŒ¨í„´ ì •ë³´ í‘œì‹œ
                if user_pattern:
                    st.info(f"ğŸ” ì‚¬ìš©ì ID íŒ¨í„´ í•„í„°ë§ ì ìš©: '{user_pattern}'")

                # S3 ë¡œê·¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
                try:
                    s3_analyzer = QCliS3LogAnalyzer(region=selected_region, logger=logger)

                    # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    from datetime import datetime, timedelta
                    start_dt = datetime.combine(start_date, datetime.min.time())
                    end_dt = datetime.combine(end_date, datetime.max.time())

                    # S3 ë¡œê·¸ ë¶„ì„ ì‹¤í–‰
                    stats = s3_analyzer.analyze_usage(
                        start_dt,
                        end_dt,
                        user_pattern if user_pattern else None
                    )

                    # ê²°ê³¼ í‘œì‹œ
                    st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("ì´ ìš”ì²­ ìˆ˜", f"{stats['total_requests']:,}")

                    with col2:
                        st.metric("Chat ìš”ì²­", f"{stats['by_type']['chat']['count']:,}")

                    with col3:
                        st.metric("Inline ì œì•ˆ", f"{stats['by_type']['inline']['count']:,}")

                    with col4:
                        st.metric("ë¶„ì„ëœ íŒŒì¼", f"{stats['total_log_files']:,}")

                    # í† í° ì‚¬ìš©ëŸ‰
                    st.header("ğŸ”¢ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰")

                    col5, col6, col7 = st.columns(3)

                    with col5:
                        st.metric("Input í† í°", f"{stats['total_input_tokens']:,}")

                    with col6:
                        st.metric("Output í† í°", f"{stats['total_output_tokens']:,}")

                    with col7:
                        st.metric("ì´ í† í°", f"{stats['total_tokens']:,}")

                    # Context Window ì‚¬ìš©ë¥ 
                    st.subheader("ğŸ“ˆ Context Window ë¶„ì„")
                    context_window = 200000
                    usage_rate = (stats['total_tokens'] / context_window) * 100

                    st.metric(
                        "ëˆ„ì  í† í° ì‚¬ìš©ë¥ ",
                        f"{usage_rate:.2f}%",
                        help=f"ì´ {stats['total_tokens']:,} í† í° / Context Window {context_window:,} í† í°"
                    )

                    # ê¸°ê°„ ì¼ìˆ˜ ê³„ì‚°
                    days_in_period = stats['period']['days']
                    daily_avg = stats['total_tokens'] / days_in_period if days_in_period > 0 else 0
                    daily_usage_rate = (daily_avg / context_window) * 100

                    col_ctx1, col_ctx2 = st.columns(2)
                    with col_ctx1:
                        st.metric("ì¼ì¼ í‰ê·  í† í°", f"{daily_avg:,.0f}")
                    with col_ctx2:
                        st.metric("ì¼ì¼ í‰ê·  ì‚¬ìš©ë¥ ", f"{daily_usage_rate:.2f}%")

                    st.info(
                        f"ğŸ’¡ **Context Window ì •ë³´**\n\n"
                        f"- Context Window: **200,000 í† í° / ì„¸ì…˜**\n"
                        f"- ëˆ„ì  ì‚¬ìš©ëŸ‰: **{stats['total_tokens']:,} í† í°** (ê¸°ê°„: {days_in_period}ì¼)\n"
                        f"- ì¼ì¼ í‰ê· : **{daily_avg:,.0f} í† í°** ({daily_usage_rate:.2f}%)\n\n"
                        f"âš ï¸ Context WindowëŠ” **ì„¸ì…˜ë³„ë¡œ ë…ë¦½ ê´€ë¦¬**ë˜ë¯€ë¡œ, ëˆ„ì  ì‚¬ìš©ë¥ ë³´ë‹¤ **ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ **ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
                    )

                    # íƒ€ì…ë³„ ìƒì„¸ ë¶„ì„
                    st.header("ğŸ“Š íƒ€ì…ë³„ ìƒì„¸ ë¶„ì„")

                    # Chat ë¶„ì„
                    st.subheader("ğŸ’¬ Chat (ëŒ€í™”)")
                    chat_stats = stats['by_type']['chat']
                    chat_avg_input = chat_stats['input_tokens'] / chat_stats['count'] if chat_stats['count'] > 0 else 0
                    chat_avg_output = chat_stats['output_tokens'] / chat_stats['count'] if chat_stats['count'] > 0 else 0
                    chat_avg_total = (chat_stats['input_tokens'] + chat_stats['output_tokens']) / chat_stats['count'] if chat_stats['count'] > 0 else 0

                    col_chat1, col_chat2, col_chat3, col_chat4 = st.columns(4)
                    with col_chat1:
                        st.metric("ìš”ì²­ ìˆ˜", f"{chat_stats['count']:,}")
                    with col_chat2:
                        st.metric("í‰ê·  ì…ë ¥", f"{chat_avg_input:.0f} í† í°")
                    with col_chat3:
                        st.metric("í‰ê·  ì¶œë ¥", f"{chat_avg_output:.0f} í† í°")
                    with col_chat4:
                        st.metric("í‰ê·  ì´í•©", f"{chat_avg_total:.0f} í† í°")

                    # Inline ë¶„ì„
                    st.subheader("âš¡ Inline ì œì•ˆ (ì½”ë“œ ìë™ì™„ì„±)")
                    inline_stats = stats['by_type']['inline']
                    inline_avg_input = inline_stats['input_tokens'] / inline_stats['count'] if inline_stats['count'] > 0 else 0
                    inline_avg_output = inline_stats['output_tokens'] / inline_stats['count'] if inline_stats['count'] > 0 else 0

                    col_inline1, col_inline2, col_inline3 = st.columns(3)
                    with col_inline1:
                        st.metric("ìš”ì²­ ìˆ˜", f"{inline_stats['count']:,}")
                    with col_inline2:
                        st.metric("í‰ê·  ì»¨í…ìŠ¤íŠ¸", f"{inline_avg_input:.0f} í† í°")
                    with col_inline3:
                        if inline_avg_output == 0:
                            st.metric("í‰ê·  ì¶œë ¥", "ë¡œê·¸ì— ì—†ìŒ", help="Inline ì œì•ˆì˜ ì‘ë‹µì€ ë¡œê·¸ì— ê¸°ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        else:
                            st.metric("í‰ê·  ì¶œë ¥", f"{inline_avg_output:.0f} í† í°")

                    # ì‚¬ìš©ìë³„ ë¶„ì„
                    if stats['by_user']:
                        st.header("ğŸ‘¥ ì‚¬ìš©ìë³„ ë¶„ì„")

                        user_data = []
                        for user_id, user_stats in stats['by_user'].items():
                            user_data.append({
                                'ì‚¬ìš©ì ID': user_id,
                                'ìš”ì²­ ìˆ˜': user_stats['requests'],
                                'Input í† í°': user_stats['input_tokens'],
                                'Output í† í°': user_stats['output_tokens'],
                                'ì´ í† í°': user_stats['input_tokens'] + user_stats['output_tokens']
                            })

                        user_df = pd.DataFrame(user_data)
                        user_df = user_df.sort_values('ì´ í† í°', ascending=False)
                        st.dataframe(user_df, use_container_width=True)

                    # ë‚ ì§œë³„ ë¶„ì„
                    if stats['by_date']:
                        st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

                        date_data = []
                        for date_str, date_stats in stats['by_date'].items():
                            date_data.append({
                                'ë‚ ì§œ': date_str,
                                'ìš”ì²­ ìˆ˜': date_stats['requests'],
                                'Input í† í°': date_stats['input_tokens'],
                                'Output í† í°': date_stats['output_tokens'],
                                'ì´ í† í°': date_stats['input_tokens'] + date_stats['output_tokens']
                            })

                        date_df = pd.DataFrame(date_data)
                        date_df = date_df.sort_values('ë‚ ì§œ')
                        st.dataframe(date_df, use_container_width=True)

                        # ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰ ì°¨íŠ¸
                        import plotly.express as px
                        fig = px.line(
                            date_df,
                            x='ë‚ ì§œ',
                            y='ì´ í† í°',
                            title='ì¼ë³„ ì´ í† í° ì‚¬ìš©ëŸ‰',
                            markers=True
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)

                    # ì‹œê°„ëŒ€ë³„ ë¶„ì„
                    if stats['by_hour']:
                        st.header("â° ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´ (UTC)")

                        hour_data = []
                        for hour, count in stats['by_hour'].items():
                            hour_int = int(hour)
                            kst_hour = (hour_int + 9) % 24
                            hour_data.append({
                                'UTC ì‹œê°„': f"{hour_int:02d}:00",
                                'KST ì‹œê°„': f"{kst_hour:02d}:00",
                                'ìš”ì²­ ìˆ˜': count
                            })

                        hour_df = pd.DataFrame(hour_data)
                        hour_df = hour_df.sort_values('UTC ì‹œê°„')

                        # í…Œì´ë¸”
                        st.dataframe(hour_df, use_container_width=True)

                        # ì‹œê°„ëŒ€ë³„ ìš”ì²­ ìˆ˜ ì°¨íŠ¸
                        fig = px.bar(
                            hour_df,
                            x='KST ì‹œê°„',
                            y='ìš”ì²­ ìˆ˜',
                            title='ì‹œê°„ëŒ€ë³„ ìš”ì²­ ìˆ˜ (í•œêµ­ ì‹œê°„)',
                            labels={'KST ì‹œê°„': 'ì‹œê°„ëŒ€', 'ìš”ì²­ ìˆ˜': 'ìš”ì²­ ìˆ˜'}
                        )
                        fig.update_xaxes(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)

                    # ê°€ìƒ ë¹„ìš© ê³„ì‚° (ì°¸ê³ ìš©)
                    st.header("ğŸ’° ê°€ìƒ ë¹„ìš© ë¶„ì„ (ì°¸ê³ ìš©)")
                    st.info(
                        "ğŸ’¡ **ì°¸ê³ **: Amazon Q Developer ProëŠ” **$19/ì›” ì •ì•¡ì œ**ì…ë‹ˆë‹¤.\n\n"
                        "ì•„ë˜ ë¹„ìš©ì€ Claude APIë¥¼ ì§ì ‘ ì‚¬ìš©í–ˆì„ ê²½ìš°ë¥¼ ê°€ì •í•œ ê°€ìƒ ë¹„ìš©ì…ë‹ˆë‹¤."
                    )

                    # Claude Sonnet 3.5 ê°€ê²© ê¸°ì¤€
                    MODEL_PRICING = {
                        "input": 0.003 / 1000,  # $0.003 per 1K tokens
                        "output": 0.015 / 1000,  # $0.015 per 1K tokens
                    }

                    virtual_cost = (
                        stats['total_input_tokens'] * MODEL_PRICING['input'] +
                        stats['total_output_tokens'] * MODEL_PRICING['output']
                    )

                    col_cost1, col_cost2, col_cost3 = st.columns(3)

                    with col_cost1:
                        st.metric("Input ë¹„ìš©", f"${stats['total_input_tokens'] * MODEL_PRICING['input']:.2f}")

                    with col_cost2:
                        st.metric("Output ë¹„ìš©", f"${stats['total_output_tokens'] * MODEL_PRICING['output']:.2f}")

                    with col_cost3:
                        st.metric("ì´ ê°€ìƒ ë¹„ìš©", f"${virtual_cost:.2f}")

                    # ROI ë¹„êµ
                    st.subheader("ğŸ“Š ROI ë¶„ì„")
                    subscription_cost = 19.0  # $19/ì›”
                    prorated_subscription = subscription_cost * (days_in_period / 30)

                    col_roi1, col_roi2, col_roi3 = st.columns(3)

                    with col_roi1:
                        st.metric("êµ¬ë…ë£Œ (ê¸°ê°„ ì¼í• )", f"${prorated_subscription:.2f}")

                    with col_roi2:
                        st.metric("ê°€ìƒ ì‚¬ìš© ë¹„ìš©", f"${virtual_cost:.2f}")

                    with col_roi3:
                        savings = virtual_cost - prorated_subscription
                        if savings > 0:
                            st.metric("ì ˆê°ì•¡", f"${savings:.2f}", delta=f"{(savings/virtual_cost)*100:.1f}% ì ˆê°")
                        else:
                            st.metric("ì†ì‹¤", f"${-savings:.2f}", delta=f"{(-savings/prorated_subscription)*100:.1f}% ì†ì‹¤", delta_color="inverse")

                except Exception as e:
                    logger.error(f"S3 ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    st.error(f"S3 ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    st.info("í”„ë¡¬í”„íŠ¸ ë¡œê¹…ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€, S3 ë²„í‚·ì— ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

        else:
            # ===== ê¸°ì¡´ Athena CSV ë¶„ì„ =====
            with st.spinner("Athenaì—ì„œ Amazon Q CLI ë°ì´í„° ë¶„ì„ ì¤‘..."):

                # ì‚¬ìš©ì íŒ¨í„´ ì •ë³´ í‘œì‹œ
                if user_pattern:
                    st.info(f"ğŸ” ì‚¬ìš©ì ID íŒ¨í„´ í•„í„°ë§ ì ìš©: '{user_pattern}'")

                # Tracker ì´ˆê¸°í™”
                tracker = QCliAthenaTracker(region=selected_region)

                # ì „ì²´ ìš”ì•½
                summary = tracker.get_total_summary(
                    start_date, end_date, user_pattern if user_pattern else None
                )

                # ì¡°íšŒ ê¸°ê°„ ì¼ìˆ˜ ê³„ì‚°
                days_in_period = (end_date - start_date).days + 1

                # ë¦¬ë°‹ ì²´í¬
                limit_check = tracker.check_official_limits(summary, days_in_period)

                # ì¶”ì„¸ ë¶„ì„
                trends = tracker.analyze_usage_trends(start_date, end_date, user_pattern if user_pattern else None)

                # ì‚¬ìš©ìë³„ ë¶„ì„
                user_df = tracker.get_user_usage_analysis(
                    start_date, end_date, user_pattern if user_pattern else None
                )

                # ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„
                feature_df = tracker.get_feature_usage_stats(
                    start_date, end_date, user_pattern if user_pattern else None
                )

                # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
                daily_df = tracker.get_daily_usage_pattern(
                    start_date, end_date, user_pattern if user_pattern else None
                )

                # í† í° ì¶”ì •
                token_estimate = tracker.estimate_tokens(summary, "average")

            st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ì±„íŒ… ë©”ì‹œì§€", f"{summary['total_chat_messages']:,}")

            with col2:
                st.metric("ì¸ë¼ì¸ ì œì•ˆ", f"{summary['total_inline_suggestions']:,}")

            with col3:
                st.metric("í™œì„± ì‚¬ìš©ì", f"{summary['unique_users']:,}")

            with col4:
                st.metric("í™œë™ ì¼ìˆ˜", f"{summary['active_days']:,}")

            col5, col6, col7, col8 = st.columns(4)

            with col5:
                st.metric("ì±„íŒ… ì½”ë“œ ë¼ì¸", f"{summary['total_chat_code_lines']:,}")

            with col6:
                st.metric("ì¸ë¼ì¸ ì½”ë“œ ë¼ì¸", f"{summary['total_inline_code_lines']:,}")

            with col7:
                st.metric("/dev ì´ë²¤íŠ¸", f"{summary['total_dev_events']:,}")

            with col8:
                st.metric("/test ì´ë²¤íŠ¸", f"{summary['total_test_events']:,}")

            # ë¦¬ë°‹ ì¶”ì  ì„¹ì…˜ (ìƒˆë¡œ ì¶”ê°€)
            st.header("âš ï¸ ê³µì‹ ë¦¬ë°‹ ëª¨ë‹ˆí„°ë§")

            # ë¦¬ë°‹ ìƒíƒœ í‘œì‹œ
            col_limit1, col_limit2 = st.columns(2)

            with col_limit1:
                st.subheader("ğŸ”§ /dev ëª…ë ¹ì–´")
                dev_limit = limit_check["dev_events"]

                # ê²½ê³  ìƒ‰ìƒ
                if dev_limit["warning"]:
                    st.error(f"âš ï¸ ê²½ê³ : ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}% ë„ë‹¬!")
                elif dev_limit["percentage"] > 50:
                    st.warning(f"ì£¼ì˜: ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}%")
                else:
                    st.success(f"ì •ìƒ: ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}%")

                st.metric(
                    "í˜„ì¬ ì‚¬ìš©ëŸ‰ / ì›”ê°„ ë¦¬ë°‹",
                    f"{dev_limit['used']} / {dev_limit['limit']}íšŒ"
                )
                st.metric(
                    "ì›”ê°„ ì˜ˆìƒ ì‚¬ìš©ëŸ‰",
                    f"{dev_limit['projected_monthly']}íšŒ",
                    delta=f"{dev_limit['projected_monthly'] - dev_limit['limit']}íšŒ ì—¬ìœ " if dev_limit['projected_monthly'] < dev_limit['limit'] else f"{dev_limit['projected_monthly'] - dev_limit['limit']}íšŒ ì´ˆê³¼ ì˜ˆìƒ"
                )

            with col_limit2:
                st.subheader("ğŸ”„ Code Transformation")
                trans_limit = limit_check["transformation_lines"]

                # ê²½ê³  ìƒ‰ìƒ
                if trans_limit["warning"]:
                    st.error(f"âš ï¸ ê²½ê³ : ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}% ë„ë‹¬!")
                elif trans_limit["percentage"] > 50:
                    st.warning(f"ì£¼ì˜: ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}%")
                else:
                    st.success(f"ì •ìƒ: ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}%")

                st.metric(
                    "í˜„ì¬ ì‚¬ìš©ëŸ‰ / ì›”ê°„ ë¦¬ë°‹",
                    f"{trans_limit['used']:,} / {trans_limit['limit']:,}ì¤„"
                )
                st.metric(
                    "ì›”ê°„ ì˜ˆìƒ ì‚¬ìš©ëŸ‰",
                    f"{trans_limit['projected_monthly']:,}ì¤„",
                    delta=f"{trans_limit['projected_monthly'] - trans_limit['limit']:,}ì¤„ ì—¬ìœ " if trans_limit['projected_monthly'] < trans_limit['limit'] else f"{trans_limit['projected_monthly'] - trans_limit['limit']:,}ì¤„ ì´ˆê³¼ ì˜ˆìƒ"
                )

            # ì‚¬ìš© íŒ¨í„´ ë¶„ì„
            st.subheader("ğŸ“ˆ ì‚¬ìš© íŒ¨í„´ ë¶„ì„")

            col_trend1, col_trend2, col_trend3 = st.columns(3)

            with col_trend1:
                st.metric("ì¼ì¼ í‰ê·  í™œë™", f"{trends['daily_avg']:.1f}ê±´")

            with col_trend2:
                st.metric("ìµœëŒ€ í™œë™ì¼", f"{trends['daily_max']:.0f}ê±´")

            with col_trend3:
                if trends["anomaly_detected"]:
                    st.error(f"âš ï¸ ì´ìƒ ê°ì§€: {trends['anomaly_count']}ì¼")
                else:
                    st.success("âœ… ì •ìƒ íŒ¨í„´")

            if trends["anomaly_detected"]:
                st.warning(
                    f"ğŸš¨ **ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ê°ì§€**: {trends['anomaly_count']}ì¼ ë™ì•ˆ ì¼í‰ê· ({trends['daily_avg']:.1f})ì˜ "
                    f"3ë°°({trends['anomaly_threshold']:.1f})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                    f"ë¦¬ë°‹ ë„ë‹¬ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤!"
                )

            # ì£¼ìš” ì•ˆë‚´ ì‚¬í•­
            st.info(
                "ğŸ’¡ **ë¦¬ë°‹ ì •ë³´**\n\n"
                "- **ì±„íŒ…/ì¸ë¼ì¸ ì œì•ˆ**: AWSê°€ ê³µì‹ ë¦¬ë°‹ì„ ê³µê°œí•˜ì§€ ì•ŠìŒ (ì¶”ì  ë¶ˆê°€)\n"
                "- **/dev ëª…ë ¹ì–´**: 30íšŒ/ì›” (ê³µì‹ ë¬¸ì„œ)\n"
                "- **Code Transformation**: 4,000ì¤„/ì›” (ê³µì‹ ë¬¸ì„œ)\n\n"
                "ğŸ“Š **ì´ ëŒ€ì‹œë³´ë“œëŠ” CSV ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°„ì ‘ ì¶”ì •**ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
                "ì‹¤ì œ ë¦¬ë°‹ ë„ë‹¬ ì‹œ AWS ì½˜ì†”ì—ì„œ 'Monthly limit reached' ë©”ì‹œì§€ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤."
            )

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì°¸ê³ ìš©ìœ¼ë¡œ ë³€ê²½)
            st.header("ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì°¸ê³ ìš©)")
            st.info(
                "ğŸ’¡ **ì°¸ê³ **: Amazon Q Developer ProëŠ” **$19/ì›” ì •ì•¡ì œ**ì…ë‹ˆë‹¤.\n\n"
                "ì•„ë˜ í† í° ì¶”ì •ì¹˜ëŠ” ì‹¤ì œ ì²­êµ¬ ë¹„ìš©ê³¼ ë¬´ê´€í•˜ë©°, ë‹¤ìŒ ìš©ë„ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤:\n"
                "- ROI ë¶„ì„: êµ¬ë…ë£Œ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë§ì´ ì‚¬ìš©í•˜ëŠ”ê°€?\n"
                "- ê°€ìƒ ë¹„êµ: Claude APIë¥¼ ì§ì ‘ ì‚¬ìš©í–ˆë‹¤ë©´ ì–¼ë§ˆê°€ ë‚˜ì™”ì„ê¹Œ?\n"
                "- ì‚¬ìš©ëŸ‰ íŒŒì•…: ëŒ€ëµì ì¸ í† í° ì‚¬ìš© ê·œëª¨ ì´í•´"
            )

            # í‰ê·  ì¶”ì •ì¹˜ë§Œ ê³„ì‚°
            token_estimate = tracker.estimate_tokens(summary, "average")

            # ê°€ìƒ ë¹„ìš© ê³„ì‚° (Claude Sonnet 3.5 ê°€ê²© ê¸°ì¤€)
            MODEL_PRICING = {
                "input": 0.003 / 1000,  # $0.003 per 1K tokens
                "output": 0.015 / 1000,  # $0.015 per 1K tokens
            }
            virtual_cost = (
                token_estimate['estimated_input_tokens'] * MODEL_PRICING['input'] +
                token_estimate['estimated_output_tokens'] * MODEL_PRICING['output']
            )

            # ì¶”ì •ì¹˜ í‘œì‹œ
            col_est1, col_est2, col_est3, col_est4 = st.columns(4)

            with col_est1:
                st.metric("Input í† í°", f"{token_estimate['estimated_input_tokens']:,}")

            with col_est2:
                st.metric("Output í† í°", f"{token_estimate['estimated_output_tokens']:,}")

            with col_est3:
                st.metric("ì´ í† í°", f"{token_estimate['estimated_total_tokens']:,}")

            with col_est4:
                st.metric("ê°€ìƒ ë¹„ìš© (Claude API)", f"${virtual_cost:.2f}")

            # ROI ë¹„êµ
            st.subheader("ğŸ’° ROI ë¶„ì„")
            subscription_cost = 19.0  # $19/ì›”
            days_in_period = (end_date - start_date).days + 1
            prorated_subscription = subscription_cost * (days_in_period / 30)

            col_roi1, col_roi2, col_roi3 = st.columns(3)

            with col_roi1:
                st.metric("êµ¬ë…ë£Œ (ê¸°ê°„ ì¼í• )", f"${prorated_subscription:.2f}")

            with col_roi2:
                st.metric("ê°€ìƒ ì‚¬ìš© ë¹„ìš©", f"${virtual_cost:.2f}")

            with col_roi3:
                savings = virtual_cost - prorated_subscription
                if savings > 0:
                    st.metric("ì ˆê°ì•¡", f"${savings:.2f}", delta=f"{(savings/virtual_cost)*100:.1f}% ì ˆê°")
                else:
                    st.metric("ì†ì‹¤", f"${-savings:.2f}", delta=f"{(-savings/prorated_subscription)*100:.1f}% ì†ì‹¤", delta_color="inverse")

            # ì‚¬ìš©ìë³„ ë¶„ì„
            st.header("ğŸ‘¥ ì‚¬ìš©ìë³„ ë¶„ì„")

            user_df = tracker.get_user_usage_analysis(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not user_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "total_chat_messages",
                    "total_inline_suggestions",
                    "total_inline_acceptances",
                    "total_chat_code_lines",
                    "total_inline_code_lines",
                    "total_dev_events",
                    "total_test_events",
                    "total_doc_events",
                    "active_days",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(user_df[col], errors="coerce").fillna(0)

                st.dataframe(user_df, use_container_width=True)

                # ì‚¬ìš©ìë³„ í™œë™ ì°¨íŠ¸
                if len(user_df) > 0:
                    import plotly.express as px

                    fig = px.bar(
                        user_df.head(10),
                        x="user_id",
                        y="total_chat_messages",
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ìë³„ ì±„íŒ… ë©”ì‹œì§€ ìˆ˜",
                        labels={"user_id": "ì‚¬ìš©ì ID", "total_chat_messages": "ì±„íŒ… ë©”ì‹œì§€ ìˆ˜"},
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)

                    # Chat vs Inline ì½”ë“œ ë¼ì¸ ë¹„êµ
                    fig2 = px.bar(
                        user_df.head(10),
                        x="user_id",
                        y=["total_chat_code_lines", "total_inline_code_lines"],
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ìë³„ ì½”ë“œ ë¼ì¸ ìƒì„± (Chat vs Inline)",
                        labels={"value": "ì½”ë“œ ë¼ì¸ ìˆ˜", "user_id": "ì‚¬ìš©ì ID"},
                        barmode="group",
                    )
                    fig2.update_xaxes(tickangle=45)
                    st.plotly_chart(fig2, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ì‚¬ìš©ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„
            st.header("ğŸ“± ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„")

            feature_df = tracker.get_feature_usage_stats(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not feature_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                for col in ["total_count", "unique_users"]:
                    if col in feature_df.columns:
                        feature_df[col] = pd.to_numeric(feature_df[col], errors="coerce").fillna(0)

                st.dataframe(feature_df, use_container_width=True)

                # ê¸°ëŠ¥ë³„ ì‚¬ìš©ëŸ‰ íŒŒì´ ì°¨íŠ¸
                if len(feature_df) > 0:
                    import plotly.express as px

                    fig = px.pie(
                        feature_df,
                        values="total_count",
                        names="feature_type",
                        title="ê¸°ëŠ¥ë³„ ì‚¬ìš© ë¹„ìœ¨",
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ê¸°ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
            st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

            daily_df = tracker.get_daily_usage_pattern(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not daily_df.empty and len(daily_df) > 0:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "total_chat_messages",
                    "total_inline_suggestions",
                    "total_inline_acceptances",
                    "total_chat_code_lines",
                    "total_inline_code_lines",
                    "unique_users",
                ]
                for col in numeric_columns:
                    if col in daily_df.columns:
                        daily_df[col] = pd.to_numeric(daily_df[col], errors="coerce").fillna(0)

                # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (MM-DD-YYYY í˜•ì‹)
                daily_df["date"] = pd.to_datetime(daily_df["date_str"], format='%m-%d-%Y')

                # í‘œì‹œìš© DataFrame ìƒì„±
                display_df = daily_df.copy()
                display_df["ë‚ ì§œ"] = display_df["date"].dt.strftime("%Y-%m-%d")
                display_df = display_df[
                    ["ë‚ ì§œ", "total_chat_messages", "total_inline_suggestions", "total_inline_acceptances", "unique_users"]
                ]
                display_df.columns = ["ë‚ ì§œ", "ì±„íŒ… ë©”ì‹œì§€", "ì¸ë¼ì¸ ì œì•ˆ", "ì¸ë¼ì¸ ìˆ˜ë½", "í™œì„± ì‚¬ìš©ì"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì¼ë³„ ì±„íŒ… ë©”ì‹œì§€ íŒ¨í„´
                fig = px.line(
                    daily_df,
                    x="date",
                    y="total_chat_messages",
                    title="ì¼ë³„ ì±„íŒ… ë©”ì‹œì§€ ìˆ˜",
                    labels={"date": "ë‚ ì§œ", "total_chat_messages": "ì±„íŒ… ë©”ì‹œì§€ ìˆ˜"},
                    markers=True,
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì¼ë³„ ì¸ë¼ì¸ ì œì•ˆ vs ìˆ˜ë½
                fig2 = go.Figure()
                fig2.add_trace(
                    go.Scatter(
                        x=daily_df["date"],
                        y=daily_df["total_inline_suggestions"],
                        mode="lines+markers",
                        name="ì¸ë¼ì¸ ì œì•ˆ",
                        line=dict(color="blue"),
                    )
                )
                fig2.add_trace(
                    go.Scatter(
                        x=daily_df["date"],
                        y=daily_df["total_inline_acceptances"],
                        mode="lines+markers",
                        name="ì¸ë¼ì¸ ìˆ˜ë½",
                        line=dict(color="green"),
                    )
                )
                fig2.update_layout(
                    title="ì¼ë³„ ì¸ë¼ì¸ ì½”ë“œ ì œì•ˆ vs ìˆ˜ë½",
                    xaxis_title="ë‚ ì§œ",
                    yaxis_title="ê°œìˆ˜",
                    hovermode="x unified",
                )
                st.plotly_chart(fig2, use_container_width=True)

                # ì¼ë³„ í™œì„± ì‚¬ìš©ì ìˆ˜
                fig3 = px.bar(
                    daily_df,
                    x="date",
                    y="unique_users",
                    title="ì¼ë³„ í™œì„± ì‚¬ìš©ì ìˆ˜",
                    labels={"date": "ë‚ ì§œ", "unique_users": "í™œì„± ì‚¬ìš©ì ìˆ˜"},
                )
                st.plotly_chart(fig3, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì¼ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        # ì´ˆê¸° í™”ë©´
        st.info(
            "ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¦¬ì „ê³¼ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•œ í›„ 'ë°ì´í„° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
        )

        st.markdown("### ğŸ› ï¸ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ")

        st.markdown("#### 1ï¸âƒ£ Amazon Q Developer ì„¤ì •")
        st.markdown(
            """
        1. **Amazon Q Developer ì½˜ì†”**ì—ì„œ "Collect granular metrics per user" ì˜µì…˜ í™œì„±í™”
        2. **S3 ë²„í‚· ì§€ì •**: ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë  S3 ë²„í‚· ì„¤ì •
        3. **ë§¤ì¼ ìì •(UTC)**ì— CSV ë¦¬í¬íŠ¸ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
        """
        )

        st.markdown("#### 2ï¸âƒ£ ë¶„ì„ í™˜ê²½ êµ¬ì¶•")
        st.code(
            """
# Amazon Q CLI ë¶„ì„ í™˜ê²½ ì„¤ì •
python setup_qcli_analytics.py --region us-east-1

# ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
streamlit run bedrock_tracker.py
        """,
            language="bash",
        )

        st.markdown("#### 3ï¸âƒ£ ë°ì´í„° ì†ŒìŠ¤")
        st.markdown(
            """
        **CSV ë¦¬í¬íŠ¸ (ì‚¬ìš©ì í™œë™)**:
        - ì¼ë³„ ì‚¬ìš©ìë³„ ìš”ì²­ ìˆ˜
        - Agentic ìš”ì²­ ìˆ˜
        - CLI/IDE ìš”ì²­ ìˆ˜
        - ì½”ë“œ ì œì•ˆ ìˆ˜
        """
        )

        st.markdown("### ğŸ“‹ ì£¼ìš” ë©”íŠ¸ë¦­")
        st.markdown(
            """
        - **ì´ ìš”ì²­ ìˆ˜**: ì „ì²´ Amazon Q ìš”ì²­ ìˆ˜
        - **Agentic ìš”ì²­**: Q&A ì±— ë˜ëŠ” agentic ì½”ë”© ìƒí˜¸ì‘ìš©
        - **CLI ìš”ì²­**: Amazon Q CLIë¥¼ í†µí•œ ìš”ì²­
        - **IDE ìš”ì²­**: IDE í”ŒëŸ¬ê·¸ì¸ì„ í†µí•œ ìš”ì²­
        - **ì½”ë“œ ì œì•ˆ**: ì½”ë“œ ìë™ ì™„ì„± ì œì•ˆ ìˆ˜
        """
        )

    logger.info("QCli Dashboard rendering complete")


if __name__ == "__main__":
    main()
