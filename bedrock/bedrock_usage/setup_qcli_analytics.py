#!/usr/bin/env python3
"""
Amazon Q CLI ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ì„¤ì •í•©ë‹ˆë‹¤:
1. S3 ë²„í‚· ìƒì„± (ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ ì €ì¥ ë° Athena ì¿¼ë¦¬ ê²°ê³¼)
2. Glue ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
3. Athena í…Œì´ë¸” ìƒì„± (CSV ë¦¬í¬íŠ¸ ë¶„ì„ìš©)
"""

import boto3
import json
import time
from datetime import datetime
import sys


def setup_qcli_analytics(region="us-east-1", recreate_table=False, create_sample_data=False):
    """Amazon Q CLI ë¶„ì„ í™˜ê²½ ì„¤ì •

    Args:
        region: AWS ë¦¬ì „
        recreate_table: Trueì¼ ê²½ìš° ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ì¬ìƒì„±
        create_sample_data: Trueì¼ ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    """

    print(f"ğŸš€ Amazon Q CLI Analytics ì„¤ì • ì‹œì‘ (ë¦¬ì „: {region})")
    print("=" * 80)

    # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    sts = boto3.client("sts", region_name=region)
    s3 = boto3.client("s3", region_name=region)
    athena = boto3.client("athena", region_name=region)
    glue = boto3.client("glue", region_name=region)

    # ê³„ì • ID ê°€ì ¸ì˜¤ê¸°
    account_id = sts.get_caller_identity()["Account"]
    print(f"ğŸ“‹ AWS ê³„ì • ID: {account_id}")

    # ë²„í‚· ì´ë¦„ ì„¤ì •
    reports_bucket = f"amazonq-developer-reports-{account_id}"
    athena_results_bucket = f"qcli-analytics-{account_id}-{region}"

    print(f"ğŸ“¦ ë¦¬í¬íŠ¸ ë²„í‚·: {reports_bucket}")
    print(f"ğŸ“¦ Athena ê²°ê³¼ ë²„í‚·: {athena_results_bucket}")

    # Step 1: S3 ë²„í‚· ìƒì„±
    print("\n" + "=" * 80)
    print("ğŸ“¦ Step 1: S3 ë²„í‚· ìƒì„±")
    print("=" * 80)

    buckets_to_create = [
        (reports_bucket, "Amazon Q Developer ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸"),
        (athena_results_bucket, "Athena ì¿¼ë¦¬ ê²°ê³¼"),
    ]

    for bucket_name, purpose in buckets_to_create:
        try:
            print(f"\nğŸ“ ë²„í‚· ìƒì„± ì¤‘: {bucket_name} ({purpose})")

            # ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            try:
                s3.head_bucket(Bucket=bucket_name)
                print(f"  âœ… ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {bucket_name}")
                continue
            except:
                pass

            # ë²„í‚· ìƒì„±
            if region == "us-east-1":
                s3.create_bucket(Bucket=bucket_name)
            else:
                s3.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={"LocationConstraint": region}
                )

            # ë²„í‚· ë²„ì €ë‹ í™œì„±í™”
            s3.put_bucket_versioning(
                Bucket=bucket_name,
                VersioningConfiguration={"Status": "Enabled"}
            )

            print(f"  âœ… ë²„í‚· ìƒì„± ì™„ë£Œ: {bucket_name}")

        except Exception as e:
            print(f"  âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨: {str(e)}")
            if "BucketAlreadyOwnedByYou" in str(e):
                print(f"  â„¹ï¸  ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
            else:
                raise

    # Step 2: Glue ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("\n" + "=" * 80)
    print("ğŸ—„ï¸  Step 2: Glue ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±")
    print("=" * 80)

    database_name = "qcli_analytics"

    try:
        glue.create_database(
            DatabaseInput={
                'Name': database_name,
                'Description': 'Amazon Q CLI ì‚¬ìš©ëŸ‰ ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤'
            }
        )
        print(f"âœ… Glue ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ: {database_name}")
    except Exception as e:
        if "AlreadyExistsException" in str(e):
            print(f"â„¹ï¸  ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {database_name}")
        else:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # Step 3: Amazon Q Developer CSV ë¦¬í¬íŠ¸ìš© í…Œì´ë¸” ìƒì„±
    print("\n" + "=" * 80)
    print("ğŸ“Š Step 3: Amazon Q Developer CSV ë¦¬í¬íŠ¸ìš© í…Œì´ë¸” ìƒì„±")
    print("=" * 80)

    # Step 3-1: ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (ì¬ìƒì„± ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
    if recreate_table:
        print(f"  ğŸ—‘ï¸  ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ ì¤‘...")
        drop_table_query = f"DROP TABLE IF EXISTS {database_name}.qcli_user_activity_reports;"

        try:
            response = athena.start_query_execution(
                QueryString=drop_table_query,
                QueryExecutionContext={'Database': database_name},
                ResultConfiguration={
                    'OutputLocation': f's3://{athena_results_bucket}/setup-results/'
                }
            )

            query_execution_id = response['QueryExecutionId']

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            for i in range(30):
                result = athena.get_query_execution(QueryExecutionId=query_execution_id)
                status = result['QueryExecution']['Status']['State']

                if status == 'SUCCEEDED':
                    print(f"  âœ… ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")
                    break
                elif status in ['FAILED', 'CANCELLED']:
                    error_msg = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                    print(f"  âš ï¸  í…Œì´ë¸” ì‚­ì œ ì‹¤íŒ¨: {error_msg}")
                    break

                time.sleep(1)

        except Exception as e:
            print(f"  âš ï¸  í…Œì´ë¸” ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

    # Step 3-2: í…Œì´ë¸” ìƒì„± (ì‹¤ì œ AWS CSV ìŠ¤í‚¤ë§ˆ)
    create_csv_table_query = f"""
    CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.qcli_user_activity_reports (
        UserId STRING,
        Date STRING,
        Chat_AICodeLines INT,
        Chat_MessagesInteracted INT,
        Chat_MessagesSent INT,
        CodeFix_AcceptanceEventCount INT,
        CodeFix_AcceptedLines INT,
        CodeFix_GeneratedLines INT,
        CodeFix_GenerationEventCount INT,
        CodeReview_FailedEventCount INT,
        CodeReview_FindingsCount INT,
        CodeReview_SucceededEventCount INT,
        Dev_AcceptanceEventCount INT,
        Dev_AcceptedLines INT,
        Dev_GeneratedLines INT,
        Dev_GenerationEventCount INT,
        DocGeneration_AcceptedFileUpdates INT,
        DocGeneration_AcceptedFilesCreations INT,
        DocGeneration_AcceptedLineAdditions INT,
        DocGeneration_AcceptedLineUpdates INT,
        DocGeneration_EventCount INT,
        DocGeneration_RejectedFileCreations INT,
        DocGeneration_RejectedFileUpdates INT,
        DocGeneration_RejectedLineAdditions INT,
        DocGeneration_RejectedLineUpdates INT,
        InlineChat_AcceptanceEventCount INT,
        InlineChat_AcceptedLineAdditions INT,
        InlineChat_AcceptedLineDeletions INT,
        InlineChat_DismissalEventCount INT,
        InlineChat_DismissedLineAdditions INT,
        InlineChat_DismissedLineDeletions INT,
        InlineChat_RejectedLineAdditions INT,
        InlineChat_RejectedLineDeletions INT,
        InlineChat_RejectionEventCount INT,
        InlineChat_TotalEventCount INT,
        Inline_AICodeLines INT,
        Inline_AcceptanceCount INT,
        Inline_SuggestionsCount INT,
        TestGeneration_AcceptedLines INT,
        TestGeneration_AcceptedTests INT,
        TestGeneration_EventCount INT,
        TestGeneration_GeneratedLines INT,
        TestGeneration_GeneratedTests INT,
        Transformation_EventCount INT,
        Transformation_LinesGenerated INT,
        Transformation_LinesIngested INT
    )
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    WITH SERDEPROPERTIES (
       'separatorChar' = ',',
       'quoteChar' = '"',
       'escapeChar' = '\\\\'
    )
    LOCATION 's3://{reports_bucket}/user-activity-reports/AWSLogs/{account_id}/QDeveloperLogs/by_user_analytic/{region}/'
    TBLPROPERTIES ('skip.header.line.count'='1');
    """

    try:
        response = athena.start_query_execution(
            QueryString=create_csv_table_query,
            QueryExecutionContext={'Database': database_name},
            ResultConfiguration={
                'OutputLocation': f's3://{athena_results_bucket}/setup-results/'
            }
        )

        query_execution_id = response['QueryExecutionId']
        print(f"  â³ Athena ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... (ID: {query_execution_id})")

        # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
        max_wait = 60
        for i in range(max_wait):
            result = athena.get_query_execution(QueryExecutionId=query_execution_id)
            status = result['QueryExecution']['Status']['State']

            if status == 'SUCCEEDED':
                print(f"  âœ… CSV ë¦¬í¬íŠ¸ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
                break
            elif status in ['FAILED', 'CANCELLED']:
                error_msg = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                print(f"  âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {error_msg}")
                break

            time.sleep(1)

    except Exception as e:
        print(f"âŒ CSV í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # Step 3-3: í…Œì´ë¸” ê²€ì¦
    print("\n  ğŸ” í…Œì´ë¸” ê²€ì¦ ì¤‘...")
    try:
        verify_query = f"SELECT COUNT(*) as row_count FROM {database_name}.qcli_user_activity_reports LIMIT 1;"

        response = athena.start_query_execution(
            QueryString=verify_query,
            QueryExecutionContext={'Database': database_name},
            ResultConfiguration={
                'OutputLocation': f's3://{athena_results_bucket}/verify-results/'
            }
        )

        query_execution_id = response['QueryExecutionId']

        # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
        for i in range(30):
            result = athena.get_query_execution(QueryExecutionId=query_execution_id)
            status = result['QueryExecution']['Status']['State']

            if status == 'SUCCEEDED':
                # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                results = athena.get_query_results(QueryExecutionId=query_execution_id)
                if len(results['ResultSet']['Rows']) > 1:
                    row_count = results['ResultSet']['Rows'][1]['Data'][0].get('VarCharValue', '0')
                    print(f"  âœ… í…Œì´ë¸” ê²€ì¦ ì™„ë£Œ - í˜„ì¬ ë°ì´í„°: {row_count}í–‰")
                else:
                    print(f"  âœ… í…Œì´ë¸” ê²€ì¦ ì™„ë£Œ - ë°ì´í„° ì—†ìŒ")
                break
            elif status in ['FAILED', 'CANCELLED']:
                error_msg = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                print(f"  âš ï¸  í…Œì´ë¸” ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
                break

            time.sleep(1)

    except Exception as e:
        print(f"  âš ï¸  í…Œì´ë¸” ê²€ì¦ ì‹¤íŒ¨: {str(e)}")

    # Step 4: ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì˜µì…˜)
    if create_sample_data:
        print("\n" + "=" * 80)
        print("ğŸ“Š Step 4: ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
        print("=" * 80)

        try:
            import csv
            import io
            from datetime import timedelta

            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ìµœê·¼ 7ì¼ê°„)
            sample_data = []
            today = datetime.now()

            users = [
                "user1@example.com",
                "user2@example.com",
                "user3@example.com",
                "developer1@company.com",
                "developer2@company.com"
            ]

            for days_ago in range(7, 0, -1):
                date = (today - timedelta(days=days_ago)).strftime("%Y-%m-%d")

                for user in users:
                    row = {
                        'date': date,
                        'user_id': user,
                        'request_count': 10 + (days_ago * 5),
                        'agentic_request_count': 3 + days_ago,
                        'code_suggestion_count': 15 + (days_ago * 3),
                        'cli_request_count': 5 + days_ago,
                        'ide_request_count': 5 + (days_ago * 2),
                        'total_tokens': 1000 + (days_ago * 100)
                    }
                    sample_data.append(row)

            # CSV ìƒì„±
            csv_buffer = io.StringIO()
            fieldnames = ['date', 'user_id', 'request_count', 'agentic_request_count',
                          'code_suggestion_count', 'cli_request_count', 'ide_request_count',
                          'total_tokens']

            writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(sample_data)

            # S3ì— ì—…ë¡œë“œ
            csv_content = csv_buffer.getvalue()
            s3_key = f"sample-user-activity-{today.strftime('%Y%m%d')}.csv"

            s3.put_object(
                Bucket=reports_bucket,
                Key=s3_key,
                Body=csv_content.encode('utf-8'),
                ContentType='text/csv'
            )
            print(f"  âœ… ìƒ˜í”Œ CSV íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {s3_key}")
            print(f"  ğŸ“Š ìƒì„±ëœ ë°ì´í„°: ìµœê·¼ 7ì¼, {len(users)}ëª… ì‚¬ìš©ì, {len(sample_data)}ê°œ ë ˆì½”ë“œ")

        except Exception as e:
            print(f"  âš ï¸  ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # ì™„ë£Œ ë©”ì‹œì§€
    print("\n" + "=" * 80)
    print("âœ¨ ì„¤ì • ì™„ë£Œ!")
    print("=" * 80)
    print(f"""
ğŸ“Œ ìƒì„±ëœ ë¦¬ì†ŒìŠ¤:
  - ë¦¬í¬íŠ¸ ë²„í‚·: {reports_bucket}
  - Athena ê²°ê³¼ ë²„í‚·: {athena_results_bucket}
  - Glue ë°ì´í„°ë² ì´ìŠ¤: {database_name}
  - Athena í…Œì´ë¸”:
      â€¢ {database_name}.qcli_user_activity_reports

ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:
  1. Amazon Q Developer ì½˜ì†”ì—ì„œ "Collect granular metrics per user" í™œì„±í™”
  2. ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ S3 ë²„í‚·ìœ¼ë¡œ {reports_bucket} ì§€ì •
  3. ëŒ€ì‹œë³´ë“œ ì‹¤í–‰:
     streamlit run bedrock_tracker.py

ğŸ’¡ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°:
  - ìƒ˜í”Œ ë°ì´í„°ê°€ í•„ìš”í•˜ë©´: python setup_qcli_analytics.py --sample-data
  - ë˜ëŠ”: python create_sample_qcli_data.py

âš ï¸  ì°¸ê³ ì‚¬í•­:
  - CSV ë¦¬í¬íŠ¸ëŠ” ë§¤ì¼ ìì •(UTC)ì— ìƒì„±ë©ë‹ˆë‹¤
  - Athena ì¿¼ë¦¬ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
  - í…Œì´ë¸” ìœ„ì¹˜: s3://{reports_bucket}/
    """)

    return {
        'reports_bucket': reports_bucket,
        'athena_results_bucket': athena_results_bucket,
        'database_name': database_name,
        'region': region
    }


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Amazon Q CLI Analytics í™˜ê²½ ì„¤ì •',
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì„¤ì •
  python setup_qcli_analytics.py

  # í…Œì´ë¸” ì¬ìƒì„±
  python setup_qcli_analytics.py --recreate-table

  # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
  python setup_qcli_analytics.py --sample-data

  # ëª¨ë“  ì˜µì…˜ ì‚¬ìš©
  python setup_qcli_analytics.py --region us-east-1 --recreate-table --sample-data
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--region', default='us-east-1',
                        help='AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)')
    parser.add_argument('--recreate-table', action='store_true',
                        help='ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ì¬ìƒì„±')
    parser.add_argument('--sample-data', action='store_true',
                        help='í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±')

    args = parser.parse_args()

    try:
        setup_qcli_analytics(
            region=args.region,
            recreate_table=args.recreate_table,
            create_sample_data=args.sample_data
        )
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
