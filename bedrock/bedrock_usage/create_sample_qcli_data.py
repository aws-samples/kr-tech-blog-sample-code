#!/usr/bin/env python3
"""
Amazon Q CLI í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
"""

import boto3
from datetime import datetime, timedelta
import csv
import io

def create_sample_qcli_data(region="us-east-1"):
    """ìƒ˜í”Œ Amazon Q CLI ì‚¬ìš©ì í™œë™ ë°ì´í„° ìƒì„±"""

    print("ğŸš€ Amazon Q CLI ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹œì‘")
    print("=" * 80)

    # AWS í´ë¼ì´ì–¸íŠ¸
    s3 = boto3.client("s3", region_name=region)
    sts = boto3.client("sts", region_name=region)

    account_id = sts.get_caller_identity()["Account"]
    bucket_name = f"amazonq-developer-reports-{account_id}"

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ìµœê·¼ 7ì¼ê°„)
    sample_data = []
    today = datetime.now()

    users = [
        "user1@example.com",
        "user2@example.com",
        "user3@example.com",
        "developer1@company.com",
        "developer2@company.com"
    ]

    for days_ago in range(7, 0, -1):
        date = (today - timedelta(days=days_ago)).strftime("%Y-%m-%d")

        for user in users:
            row = {
                'date': date,
                'user_id': user,
                'request_count': 10 + (days_ago * 5),
                'agentic_request_count': 3 + days_ago,
                'code_suggestion_count': 15 + (days_ago * 3),
                'cli_request_count': 5 + days_ago,
                'ide_request_count': 5 + (days_ago * 2),
                'total_tokens': 1000 + (days_ago * 100)
            }
            sample_data.append(row)

    # CSV ìƒì„±
    csv_buffer = io.StringIO()
    fieldnames = ['date', 'user_id', 'request_count', 'agentic_request_count',
                  'code_suggestion_count', 'cli_request_count', 'ide_request_count',
                  'total_tokens']

    writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(sample_data)

    # S3ì— ì—…ë¡œë“œ
    csv_content = csv_buffer.getvalue()
    s3_key = f"sample-user-activity-{today.strftime('%Y%m%d')}.csv"

    try:
        s3.put_object(
            Bucket=bucket_name,
            Key=s3_key,
            Body=csv_content.encode('utf-8'),
            ContentType='text/csv'
        )
        print(f"âœ… ìƒ˜í”Œ CSV íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ:")
        print(f"   s3://{bucket_name}/{s3_key}")
        print(f"\nğŸ“Š ìƒì„±ëœ ë°ì´í„°:")
        print(f"   - ê¸°ê°„: ìµœê·¼ 7ì¼")
        print(f"   - ì‚¬ìš©ì ìˆ˜: {len(users)}ëª…")
        print(f"   - ì´ ë ˆì½”ë“œ: {len(sample_data)}ê°œ")
        print(f"\nğŸ’¡ ì´ì œ Streamlit ëŒ€ì‹œë³´ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print(f"   streamlit run bedrock_tracker.py")

    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Amazon Q CLI ìƒ˜í”Œ ë°ì´í„° ìƒì„±')
    parser.add_argument('--region', default='us-east-1', help='AWS ë¦¬ì „')

    args = parser.parse_args()

    try:
        create_sample_qcli_data(region=args.region)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
