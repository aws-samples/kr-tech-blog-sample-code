#!/usr/bin/env python3
"""
CLI ë²„ì „ì˜ Bedrock Usage Tracker - Athena ê¸°ë°˜
í„°ë¯¸ë„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ì–‘í•œ ë¶„ì„ ê¸°ëŠ¥ ì œê³µ
"""

import boto3
import pandas as pd
from datetime import datetime, timedelta
import argparse
import time
from typing import Dict
import logging
from pathlib import Path
import json
import sys

# S3 ë¡œê·¸ ë¶„ì„ê¸° ì„í¬íŠ¸
from qcli_s3_analyzer import QCliS3LogAnalyzer

# ë¡œê¹… ì„¤ì •
def setup_logger():
    """ë””ë²„ê¹…ìš© ë¡œê±° ì„¤ì •"""
    log_dir = Path(__file__).parent / 'log'
    log_dir.mkdir(exist_ok=True)

    log_filename = log_dir / f"bedrock_tracker_cli_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    logger = logging.getLogger('BedrockTrackerCLI')
    logger.setLevel(logging.DEBUG)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.info(f"Logger initialized. Log file: {log_filename}")

    return logger

# ê¸€ë¡œë²Œ ë¡œê±°
logger = setup_logger()

# AWS Bedrock ëª¨ë¸ ê°€ê²© í…Œì´ë¸” (ë¦¬ì „ë³„)
# ì°¸ê³ : ìµœì‹  ê°€ê²©ì€ https://aws.amazon.com/bedrock/pricing/ ì—ì„œ í™•ì¸í•˜ì„¸ìš”
# ê°€ê²©ì€ USD ê¸°ì¤€ì´ë©°, 1000 í† í°ë‹¹ ê°€ê²©ì…ë‹ˆë‹¤
MODEL_PRICING = {
    # ê¸°ë³¸ ê°€ê²© (ëŒ€ë¶€ë¶„ì˜ ë¦¬ì „ì— ì ìš©)
    "default": {
        # Claude 3 ëª¨ë¸
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        # Claude 3.5 ëª¨ë¸
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        # Claude 3.7 ëª¨ë¸
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        # Claude 4 ëª¨ë¸
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US East (N. Virginia) - us-east-1
    "us-east-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US West (Oregon) - us-west-2
    "us-west-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Europe (Frankfurt) - eu-central-1
    "eu-central-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Tokyo) - ap-northeast-1
    "ap-northeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Seoul) - ap-northeast-2
    "ap-northeast-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Singapore) - ap-southeast-1
    "ap-southeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
}

# ë¦¬ì „ ì„¤ì •
REGIONS = {
    "us-east-1": "US East (N. Virginia)",
    "us-west-2": "US West (Oregon)",
    "eu-central-1": "Europe (Frankfurt)",
    "ap-northeast-1": "Asia Pacific (Tokyo)",
    "ap-northeast-2": "Asia Pacific (Seoul)",
    "ap-southeast-1": "Asia Pacific (Singapore)",
}

def get_model_cost(model_id: str, input_tokens: int, output_tokens: int, region: str = "default") -> float:
    """ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚° (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        model_id: Bedrock ëª¨ë¸ ID (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0)
        input_tokens: ì…ë ¥ í† í° ìˆ˜
        output_tokens: ì¶œë ¥ í† í° ìˆ˜
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        float: ê³„ì‚°ëœ ë¹„ìš© (USD)
    """
    logger.debug(
        f"Calculating cost for model: {model_id}, input: {input_tokens}, output: {output_tokens}, region: {region}"
    )

    # ëª¨ë¸ IDì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0 -> claude-3-haiku-20240307)
    model_name = model_id.split('.')[-1].split('-v')[0] if '.' in model_id else model_id

    # ë¦¬ì „ë³„ ê°€ê²© í…Œì´ë¸” ì„ íƒ (í•´ë‹¹ ë¦¬ì „ì´ ì—†ìœ¼ë©´ default ì‚¬ìš©)
    region_pricing = MODEL_PRICING.get(region, MODEL_PRICING["default"])

    # ê°€ê²© í…Œì´ë¸”ì—ì„œ ëª¨ë¸ ì°¾ê¸°
    for key, pricing in region_pricing.items():
        if key in model_name:
            # ê°€ê²©ì€ 1000 í† í°ë‹¹ ê°€ê²©ì´ë¯€ë¡œ 1000ìœ¼ë¡œ ë‚˜ëˆ”
            cost = (input_tokens * pricing["input"] / 1000) + (
                output_tokens * pricing["output"] / 1000
            )
            logger.debug(f"Model: {key}, Region: {region}, Cost: ${cost:.6f}")
            return cost

    # ê¸°ë³¸ ê°€ê²© (Claude 3 Haiku)
    logger.warning(f"Unknown model: {model_id}, using default pricing (Claude 3 Haiku)")
    default_pricing = MODEL_PRICING["default"]["claude-3-haiku-20240307"]
    default_cost = (input_tokens * default_pricing["input"] / 1000) + (
        output_tokens * default_pricing["output"] / 1000
    )
    return default_cost


class BedrockAthenaTracker:
    def __init__(self, region='us-east-1'):
        logger.info(f"Initializing BedrockAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client('athena', region_name=region)
        sts_client = boto3.client('sts', region_name=region)
        self.account_id = sts_client.get_caller_identity()['Account']
        # bedrock_tracker.pyì™€ ë™ì¼í•œ ë²„í‚·ëª… ì‚¬ìš©
        self.results_bucket = f'bedrock-analytics-{self.account_id}-{self.region}'
        logger.info(f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}")

    def get_current_logging_config(self) -> Dict:
        """í˜„ì¬ ì„¤ì •ëœ Model Invocation Logging ì •ë³´ ì¡°íšŒ"""
        logger.info("Getting current logging configuration")
        try:
            bedrock = boto3.client('bedrock', region_name=self.region)
            response = bedrock.get_model_invocation_logging_configuration()

            if 'loggingConfig' in response:
                config = response['loggingConfig']

                if 's3Config' in config:
                    result = {
                        'type': 's3',
                        'bucket': config['s3Config'].get('bucketName', ''),
                        'prefix': config['s3Config'].get('keyPrefix', ''),
                        'status': 'enabled'
                    }
                    logger.info(f"Logging config: {result}")
                    return result

            logger.warning("Logging is disabled")
            return {'status': 'disabled'}

        except Exception as e:
            logger.error(f"Error getting logging config: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def execute_athena_query(self, query: str, database: str = 'bedrock_analytics') -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={'Database': database},
                ResultConfiguration={
                    'OutputLocation': f's3://{self.results_bucket}/query-results/'
                }
            )

            query_id = response['QueryExecutionId']
            logger.info(f"Query execution started: {query_id}")

            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result['QueryExecution']['Status']['State']

                if status == 'SUCCEEDED':
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ['FAILED', 'CANCELLED']:
                    error = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            columns = [col['Label'] for col in result_response['ResultSet']['ResultSetMetadata']['ColumnInfo']]
            rows = []

            for row in result_response['ResultSet']['Rows'][1:]:
                row_data = [field.get('VarCharValue', '') for field in row['Data']]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            print(f"âŒ Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", file=sys.stderr)
            return pd.DataFrame()

    def get_total_summary(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„"""
        logger.info(f"Getting total summary from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            COUNT(*) as total_calls,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty:
            result = {
                'total_calls': int(df.iloc[0]['total_calls']) if df.iloc[0]['total_calls'] else 0,
                'total_input_tokens': int(df.iloc[0]['total_input_tokens']) if df.iloc[0]['total_input_tokens'] else 0,
                'total_output_tokens': int(df.iloc[0]['total_output_tokens']) if df.iloc[0]['total_output_tokens'] else 0,
                'total_cost_usd': 0.0
            }
            logger.info(f"Total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {'total_calls': 0, 'total_input_tokens': 0, 'total_output_tokens': 0, 'total_cost_usd': 0.0}

    def get_user_cost_analysis(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ë¹„ìš© ë¶„ì„"""
        logger.info(f"Getting user cost analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_user_app_detail_analysis(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> pd.DataFrame:
        """ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„"""
        logger.info(f"Getting user-app detail analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn, modelId
        ORDER BY user_or_app, call_count DESC
        """

        return self.execute_athena_query(query)

    def get_model_usage_stats(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> pd.DataFrame:
        """ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„"""
        logger.info(f"Getting model usage stats from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            AVG(CAST(input.inputTokenCount AS DOUBLE)) as avg_input_tokens,
            AVG(CAST(output.outputTokenCount AS DOUBLE)) as avg_output_tokens,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY modelId
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(f"Getting daily usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year, month, day,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day
        ORDER BY year, month, day
        """

        return self.execute_athena_query(query)

    def get_hourly_usage_pattern(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> pd.DataFrame:
        """ì‹œê°„ë³„ ì‚¬ìš© íŒ¨í„´ - timestampì—ì„œ hour ì¶”ì¶œ"""
        logger.info(f"Getting hourly usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year,
            month,
            day,
            date_format(from_iso8601_timestamp(timestamp), '%H') as hour,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        ORDER BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        """

        return self.execute_athena_query(query)


# QCli í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • ìƒìˆ˜
# ê¸°ì¤€: ì˜ì–´ ë‹¨ì–´ 1.4í† í°, 4ê¸€ìë‹¹ 5í† í°
# ì½”ë“œ 1ì¤„ í‰ê·  60-80ë¬¸ì = ì•½ 75-100í† í°
QCLI_TOKEN_ESTIMATION = {
    "conservative": {  # ë³´ìˆ˜ì  ì¶”ì • (ì§§ì€ ì½”ë“œ/ê°„ë‹¨í•œ ì§ˆë¬¸)
        "chat_message_input": 100,
        "chat_message_output": 300,
        "chat_code_line": 50,
        "inline_suggestion": 40,
        "inline_code_line": 50,
        "dev_event_input": 400,
        "dev_event_output": 600,
        "test_event_input": 300,
        "test_event_output": 500,
        "doc_event_input": 200,
        "doc_event_output": 400,
    },
    "average": {  # í‰ê·  ì¶”ì • (ì¼ë°˜ì ì¸ ì‚¬ìš©) - ê¶Œì¥
        "chat_message_input": 150,
        "chat_message_output": 500,
        "chat_code_line": 75,
        "inline_suggestion": 60,
        "inline_code_line": 75,
        "dev_event_input": 600,
        "dev_event_output": 1000,
        "test_event_input": 450,
        "test_event_output": 750,
        "doc_event_input": 350,
        "doc_event_output": 600,
    },
    "optimistic": {  # ë‚™ê´€ì  ì¶”ì • (ë³µì¡í•œ ì½”ë“œ/ê¸´ ëŒ€í™”)
        "chat_message_input": 200,
        "chat_message_output": 800,
        "chat_code_line": 100,
        "inline_suggestion": 80,
        "inline_code_line": 100,
        "dev_event_input": 1000,
        "dev_event_output": 1500,
        "test_event_input": 600,
        "test_event_output": 1000,
        "doc_event_input": 500,
        "doc_event_output": 800,
    }
}


class QCliAthenaTracker:
    """Amazon Q CLI ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ Athena ì¿¼ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, region='us-east-1'):
        logger.info(f"Initializing QCliAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        self.results_bucket = f"amazonq-developer-reports-{self.account_id}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def execute_athena_query(
        self, query: str, database: str = "qcli_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            print(f"âŒ Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", file=sys.stderr)
            return pd.DataFrame()

    def get_total_summary(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„ - Amazon Q Developer CSV ë¦¬í¬íŠ¸ ê¸°ë°˜"""
        logger.info(
            f"Getting QCli total summary from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        # ì‹¤ì œ AWS CSV ë©”íŠ¸ë¦­ ì‚¬ìš©:
        # - Chat_MessagesSent: ì±„íŒ… ë©”ì‹œì§€ ìˆ˜
        # - Inline_SuggestionsCount: ì¸ë¼ì¸ ì½”ë“œ ì œì•ˆ ìˆ˜
        # - Chat_MessagesInteracted: ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë©”íŠ¸ë¦­
        query = f"""
        SELECT
            COUNT(DISTINCT UserId) as unique_users,
            COUNT(DISTINCT Date) as active_days,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_dev_events,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_test_events
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty and df.iloc[0]["unique_users"]:
            result = {
                "unique_users": (
                    int(df.iloc[0]["unique_users"])
                    if df.iloc[0]["unique_users"]
                    else 0
                ),
                "active_days": (
                    int(df.iloc[0]["active_days"]) if df.iloc[0]["active_days"] else 0
                ),
                "total_chat_messages": (
                    int(df.iloc[0]["total_chat_messages"])
                    if df.iloc[0]["total_chat_messages"]
                    else 0
                ),
                "total_inline_suggestions": (
                    int(df.iloc[0]["total_inline_suggestions"])
                    if df.iloc[0]["total_inline_suggestions"]
                    else 0
                ),
                "total_inline_acceptances": (
                    int(df.iloc[0]["total_inline_acceptances"])
                    if df.iloc[0]["total_inline_acceptances"]
                    else 0
                ),
                "total_chat_code_lines": (
                    int(df.iloc[0]["total_chat_code_lines"])
                    if df.iloc[0]["total_chat_code_lines"]
                    else 0
                ),
                "total_inline_code_lines": (
                    int(df.iloc[0]["total_inline_code_lines"])
                    if df.iloc[0]["total_inline_code_lines"]
                    else 0
                ),
                "total_dev_events": (
                    int(df.iloc[0]["total_dev_events"])
                    if df.iloc[0]["total_dev_events"]
                    else 0
                ),
                "total_test_events": (
                    int(df.iloc[0]["total_test_events"])
                    if df.iloc[0]["total_test_events"]
                    else 0
                ),
            }
            logger.info(f"QCli total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "unique_users": 0,
                "active_days": 0,
                "total_chat_messages": 0,
                "total_inline_suggestions": 0,
                "total_inline_acceptances": 0,
                "total_chat_code_lines": 0,
                "total_inline_code_lines": 0,
                "total_dev_events": 0,
                "total_test_events": 0,
            }

    def get_user_usage_analysis(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        logger.info(
            f"Getting QCli user usage analysis from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            UserId as user_id,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_dev_events,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_test_events,
            SUM(CAST(DocGeneration_EventCount AS BIGINT)) as total_doc_events,
            COUNT(DISTINCT Date) as active_days,
            MIN(Date) as first_activity,
            MAX(Date) as last_activity
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        GROUP BY UserId
        ORDER BY total_chat_messages DESC
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(
            f"Getting QCli daily usage pattern from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            Date as date_str,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_chat_messages,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_inline_suggestions,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_inline_acceptances,
            SUM(CAST(Chat_AICodeLines AS BIGINT)) as total_chat_code_lines,
            SUM(CAST(Inline_AICodeLines AS BIGINT)) as total_inline_code_lines,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        GROUP BY Date
        ORDER BY Date
        """

        return self.execute_athena_query(query)


    def get_feature_usage_stats(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„ (Chat, Inline, Dev, Test, Doc ë“±)"""
        logger.info(
            f"Getting QCli feature usage stats from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND UserId LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            'Chat Messages' as feature_type,
            SUM(CAST(Chat_MessagesSent AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            'Inline Suggestions' as feature_type,
            SUM(CAST(Inline_SuggestionsCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            'Inline Acceptances' as feature_type,
            SUM(CAST(Inline_AcceptanceCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/dev Events' as feature_type,
            SUM(CAST(Dev_GenerationEventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/test Events' as feature_type,
            SUM(CAST(TestGeneration_EventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        UNION ALL
        SELECT
            '/doc Events' as feature_type,
            SUM(CAST(DocGeneration_EventCount AS BIGINT)) as total_count,
            COUNT(DISTINCT UserId) as unique_users
        FROM qcli_user_activity_reports
        WHERE parse_datetime(Date, 'MM-dd-yyyy') BETWEEN parse_datetime('{start_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            AND parse_datetime('{end_date.strftime('%m-%d-%Y')}', 'MM-dd-yyyy')
            {user_filter}
        ORDER BY total_count DESC
        """

        return self.execute_athena_query(query)

    def estimate_tokens(self, summary: Dict, estimation_type: str = "average") -> Dict:
        """ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¡œë¶€í„° í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •

        Args:
            summary: get_total_summary()ì—ì„œ ë°˜í™˜ëœ ìš”ì•½ ë°ì´í„°
            estimation_type: "conservative", "average", "optimistic" ì¤‘ ì„ íƒ

        Returns:
            Dict: ì¶”ì •ëœ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
        """
        logger.info(f"Estimating tokens with {estimation_type} model")

        if estimation_type not in QCLI_TOKEN_ESTIMATION:
            logger.warning(f"Unknown estimation type: {estimation_type}, using 'average'")
            estimation_type = "average"

        constants = QCLI_TOKEN_ESTIMATION[estimation_type]

        # Input í† í° ì¶”ì •
        estimated_input_tokens = (
            summary.get("total_chat_messages", 0) * constants["chat_message_input"] +
            summary.get("total_inline_suggestions", 0) * constants["inline_suggestion"] +
            summary.get("total_dev_events", 0) * constants["dev_event_input"] +
            summary.get("total_test_events", 0) * constants["test_event_input"] +
            (summary.get("total_doc_events", 0) if "total_doc_events" in summary else 0) * constants["doc_event_input"]
        )

        # Output í† í° ì¶”ì •
        estimated_output_tokens = (
            summary.get("total_chat_messages", 0) * constants["chat_message_output"] +
            summary.get("total_chat_code_lines", 0) * constants["chat_code_line"] +
            summary.get("total_inline_code_lines", 0) * constants["inline_code_line"] +
            summary.get("total_inline_acceptances", 0) * constants["inline_suggestion"] +
            summary.get("total_dev_events", 0) * constants["dev_event_output"] +
            summary.get("total_test_events", 0) * constants["test_event_output"] +
            (summary.get("total_doc_events", 0) if "total_doc_events" in summary else 0) * constants["doc_event_output"]
        )

        total_tokens = estimated_input_tokens + estimated_output_tokens

        result = {
            "estimation_type": estimation_type,
            "estimated_input_tokens": int(estimated_input_tokens),
            "estimated_output_tokens": int(estimated_output_tokens),
            "estimated_total_tokens": int(total_tokens),
        }

        logger.info(f"Token estimation result: {result}")
        return result

    def check_official_limits(self, summary: Dict, days_in_period: int) -> Dict:
        """ê³µì‹ ë¦¬ë°‹ ì²´í¬ ë° ê²½ê³  ìƒì„±

        Args:
            summary: get_total_summary()ì—ì„œ ë°˜í™˜ëœ ìš”ì•½ ë°ì´í„°
            days_in_period: ì¡°íšŒ ê¸°ê°„ì˜ ì¼ìˆ˜

        Returns:
            Dict: ë¦¬ë°‹ ì²´í¬ ê²°ê³¼ ë° ê²½ê³ 
        """
        logger.info("Checking official limits")

        # ê³µì‹ ë¬¸ì„œí™”ëœ ë¦¬ë°‹
        OFFICIAL_LIMITS = {
            "dev_events": 30,  # /dev ëª…ë ¹ì–´: 30íšŒ/ì›”
            "transformation_lines": 4000,  # Code Transformation: 4,000ì¤„/ì›”
            # ì±„íŒ…/ì¸ë¼ì¸: AWSê°€ ê³µê°œí•˜ì§€ ì•ŠìŒ
        }

        # ì›”ê°„ ì‚¬ìš©ëŸ‰ ì¶”ì • (í˜„ì¬ ê¸°ê°„ì„ 30ì¼ë¡œ í™˜ì‚°)
        monthly_factor = 30 / days_in_period if days_in_period > 0 else 1

        dev_events_used = summary.get("total_dev_events", 0)
        dev_events_projected = int(dev_events_used * monthly_factor)

        # Transformation ë°ì´í„°ëŠ” summaryì— ì—†ì„ ìˆ˜ ìˆìŒ (ì¶”í›„ ì¶”ê°€ ê°€ëŠ¥)
        transformation_lines_used = summary.get("total_transformation_lines", 0)
        transformation_lines_projected = int(transformation_lines_used * monthly_factor)

        result = {
            "dev_events": {
                "used": dev_events_used,
                "limit": OFFICIAL_LIMITS["dev_events"],
                "projected_monthly": dev_events_projected,
                "percentage": (dev_events_projected / OFFICIAL_LIMITS["dev_events"] * 100) if OFFICIAL_LIMITS["dev_events"] > 0 else 0,
                "warning": dev_events_projected >= OFFICIAL_LIMITS["dev_events"] * 0.8
            },
            "transformation_lines": {
                "used": transformation_lines_used,
                "limit": OFFICIAL_LIMITS["transformation_lines"],
                "projected_monthly": transformation_lines_projected,
                "percentage": (transformation_lines_projected / OFFICIAL_LIMITS["transformation_lines"] * 100) if OFFICIAL_LIMITS["transformation_lines"] > 0 else 0,
                "warning": transformation_lines_projected >= OFFICIAL_LIMITS["transformation_lines"] * 0.8
            }
        }

        logger.info(f"Limit check result: {result}")
        return result

    def analyze_usage_trends(self, start_date: datetime, end_date: datetime, user_pattern: str = None) -> Dict:
        """ì‚¬ìš©ëŸ‰ ì¶”ì„¸ ë¶„ì„ ë° ì´ìƒ ê°ì§€

        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            user_pattern: ì‚¬ìš©ì í•„í„° íŒ¨í„´

        Returns:
            Dict: ì¶”ì„¸ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"Analyzing usage trends from {start_date} to {end_date}")

        # ì¼ë³„ ì‚¬ìš© íŒ¨í„´ ì¡°íšŒ
        daily_df = self.get_daily_usage_pattern(start_date, end_date, user_pattern)

        if daily_df.empty:
            return {
                "daily_avg": 0,
                "daily_max": 0,
                "daily_min": 0,
                "anomaly_detected": False
            }

        # ìˆ«ì ë³€í™˜
        for col in ["total_chat_messages", "total_inline_suggestions"]:
            if col in daily_df.columns:
                daily_df[col] = pd.to_numeric(daily_df[col], errors='coerce').fillna(0)

        # ì¼ì¼ ì´ í™œë™ ê³„ì‚°
        daily_df["total_activity"] = (
            daily_df.get("total_chat_messages", 0) +
            daily_df.get("total_inline_suggestions", 0)
        )

        daily_avg = daily_df["total_activity"].mean()
        daily_max = daily_df["total_activity"].max()
        daily_min = daily_df["total_activity"].min()

        # ì´ìƒ ê°ì§€: ì¼í‰ê· ì˜ 3ë°° ì´ˆê³¼í•˜ëŠ” ë‚ ì´ ìˆëŠ”ì§€
        anomaly_threshold = daily_avg * 3
        anomaly_days = daily_df[daily_df["total_activity"] > anomaly_threshold]

        result = {
            "daily_avg": float(daily_avg),
            "daily_max": float(daily_max),
            "daily_min": float(daily_min),
            "anomaly_detected": len(anomaly_days) > 0,
            "anomaly_count": len(anomaly_days),
            "anomaly_threshold": float(anomaly_threshold)
        }

        logger.info(f"Trend analysis result: {result}")
        return result


def calculate_cost_for_dataframe(df: pd.DataFrame, model_col: str = 'model_name', region: str = "default") -> pd.DataFrame:
    """DataFrameì— ë¹„ìš© ì»¬ëŸ¼ ì¶”ê°€ (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        df: ë¹„ìš©ì„ ê³„ì‚°í•  DataFrame
        model_col: ëª¨ë¸ëª…ì´ ìˆëŠ” ì»¬ëŸ¼ëª…
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        pd.DataFrame: ë¹„ìš© ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    logger.info(f"Calculating cost for DataFrame with {len(df)} rows, region: {region}")

    if df.empty:
        return df

    costs = []
    for _, row in df.iterrows():
        model = row.get(model_col, '')
        input_tokens = int(row.get('total_input_tokens', 0)) if row.get('total_input_tokens') else 0
        output_tokens = int(row.get('total_output_tokens', 0)) if row.get('total_output_tokens') else 0
        cost = get_model_cost(model, input_tokens, output_tokens, region)
        costs.append(cost)

    df['estimated_cost_usd'] = costs
    logger.info(f"Total cost calculated for region {region}: ${sum(costs):.4f}")
    return df


def print_summary(summary: Dict):
    """ì „ì²´ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ì „ì²´ ìš”ì•½".center(80))
    print("="*80)
    print(f"  ì´ API í˜¸ì¶œ:       {summary['total_calls']:>15,}")
    print(f"  ì´ Input í† í°:     {summary['total_input_tokens']:>15,}")
    print(f"  ì´ Output í† í°:    {summary['total_output_tokens']:>15,}")
    print(f"  ì´ ë¹„ìš© (USD):     ${summary['total_cost_usd']:>14.4f}")
    print("="*80 + "\n")


def print_dataframe_table(df: pd.DataFrame, title: str, max_rows: int = 20):
    """DataFrameì„ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    if df.empty:
        print(f"\n{title}: ë°ì´í„° ì—†ìŒ\n")
        return

    print(f"\n{'='*80}")
    print(f"{title}".center(80))
    print("="*80)

    # pandas ì¶œë ¥ ì˜µì…˜ ì„¤ì •
    pd.set_option('display.max_rows', max_rows)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 1000)
    pd.set_option('display.colheader_justify', 'left')

    print(df.head(max_rows).to_string(index=False))

    if len(df) > max_rows:
        print(f"\n... ({len(df) - max_rows} more rows)")

    print("="*80 + "\n")


def save_to_csv(df: pd.DataFrame, filename: str):
    """CSVë¡œ ì €ì¥"""
    report_dir = Path(__file__).parent / 'report'
    report_dir.mkdir(exist_ok=True)

    filepath = report_dir / filename
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥: {filepath}")


def save_to_json(data: dict, filename: str):
    """JSONìœ¼ë¡œ ì €ì¥"""
    report_dir = Path(__file__).parent / 'report'
    report_dir.mkdir(exist_ok=True)

    filepath = report_dir / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… JSON ì €ì¥: {filepath}")


def print_qcli_summary(summary: Dict, token_estimates: Dict, limit_check: Dict = None, trends: Dict = None):
    """QCli ì „ì²´ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š Amazon Q CLI ì „ì²´ ìš”ì•½".center(80))
    print("="*80)
    print(f"  í™œì„± ì‚¬ìš©ì:           {summary['unique_users']:>15,}")
    print(f"  í™œë™ ì¼ìˆ˜:             {summary['active_days']:>15,}")
    print(f"  ì±„íŒ… ë©”ì‹œì§€:           {summary['total_chat_messages']:>15,}")
    print(f"  ì¸ë¼ì¸ ì œì•ˆ:           {summary['total_inline_suggestions']:>15,}")
    print(f"  ì¸ë¼ì¸ ìˆ˜ë½:           {summary['total_inline_acceptances']:>15,}")
    print(f"  ì±„íŒ… ì½”ë“œ ë¼ì¸:        {summary['total_chat_code_lines']:>15,}")
    print(f"  ì¸ë¼ì¸ ì½”ë“œ ë¼ì¸:      {summary['total_inline_code_lines']:>15,}")
    print(f"  /dev ì´ë²¤íŠ¸:           {summary['total_dev_events']:>15,}")
    print(f"  /test ì´ë²¤íŠ¸:          {summary['total_test_events']:>15,}")
    print("="*80 + "\n")

    # í† í° ì¶”ì •ì¹˜ ì¶œë ¥ (í‰ê· ë§Œ)
    print("\n" + "="*80)
    print("ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì°¸ê³ ìš©)".center(80))
    print("="*80)
    print("ğŸ’¡ ì°¸ê³ : Amazon Q Developer ProëŠ” $19/ì›” ì •ì•¡ì œì…ë‹ˆë‹¤.")
    print("   ì•„ë˜ í† í° ì¶”ì •ì¹˜ëŠ” ì‹¤ì œ ì²­êµ¬ ë¹„ìš©ê³¼ ë¬´ê´€í•˜ë©° ROI ë¶„ì„ ìš©ë„ì…ë‹ˆë‹¤.\n")

    # í‰ê·  ì¶”ì •ì¹˜ë§Œ ì‚¬ìš©
    token_avg = token_estimates.get("average", token_estimates[list(token_estimates.keys())[0]])

    print(f"  Input í† í°:       {token_avg['estimated_input_tokens']:>15,}")
    print(f"  Output í† í°:      {token_avg['estimated_output_tokens']:>15,}")
    print(f"  ì´ í† í°:          {token_avg['estimated_total_tokens']:>15,}")

    # ê°€ìƒ ë¹„ìš© ê³„ì‚° (Claude Sonnet 3.5 ê°€ê²© ê¸°ì¤€)
    MODEL_PRICING = {
        "input": 0.003 / 1000,
        "output": 0.015 / 1000,
    }
    virtual_cost = (
        token_avg['estimated_input_tokens'] * MODEL_PRICING['input'] +
        token_avg['estimated_output_tokens'] * MODEL_PRICING['output']
    )
    print(f"  ê°€ìƒ ë¹„ìš©:        ${virtual_cost:>14.2f}")
    print()

    # ROI ë¹„êµ
    print("ğŸ’° ROI ë¶„ì„:")
    subscription_cost = 19.0
    days_in_period = (summary.get('active_days', 30))
    prorated_subscription = subscription_cost * (days_in_period / 30)

    print(f"  êµ¬ë…ë£Œ (ì¼í• ):    ${prorated_subscription:>14.2f}")
    print(f"  ê°€ìƒ ì‚¬ìš© ë¹„ìš©:   ${virtual_cost:>14.2f}")

    savings = virtual_cost - prorated_subscription
    if savings > 0:
        savings_pct = (savings / virtual_cost) * 100
        print(f"  ì ˆê°ì•¡:           ${savings:>14.2f} ({savings_pct:.1f}% ì ˆê°)")
    else:
        loss_pct = (-savings / prorated_subscription) * 100
        print(f"  ì†ì‹¤:             ${-savings:>14.2f} ({loss_pct:.1f}% ì†ì‹¤)")

    print("="*80 + "\n")

    # ë¦¬ë°‹ ì²´í¬ ì¶œë ¥
    if limit_check:
        print("\n" + "="*80)
        print("âš ï¸ ê³µì‹ ë¦¬ë°‹ ëª¨ë‹ˆí„°ë§".center(80))
        print("="*80)

        dev_limit = limit_check["dev_events"]
        trans_limit = limit_check["transformation_lines"]

        print("\nğŸ”§ /dev ëª…ë ¹ì–´:")
        print(f"   í˜„ì¬ ì‚¬ìš©ëŸ‰:  {dev_limit['used']:>3} / {dev_limit['limit']:>3}íšŒ")
        print(f"   ì›”ê°„ ì˜ˆìƒ:    {dev_limit['projected_monthly']:>3}íšŒ ({dev_limit['percentage']:.1f}%)")
        if dev_limit['warning']:
            print(f"   âš ï¸  ê²½ê³ : ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}% ë„ë‹¬!")
        elif dev_limit['percentage'] > 50:
            print(f"   âš ï¸  ì£¼ì˜: ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}%")
        else:
            print(f"   âœ… ì •ìƒ: ì›”ê°„ ë¦¬ë°‹ì˜ {dev_limit['percentage']:.1f}%")

        print("\nğŸ”„ Code Transformation:")
        print(f"   í˜„ì¬ ì‚¬ìš©ëŸ‰:  {trans_limit['used']:>5,} / {trans_limit['limit']:>5,}ì¤„")
        print(f"   ì›”ê°„ ì˜ˆìƒ:    {trans_limit['projected_monthly']:>5,}ì¤„ ({trans_limit['percentage']:.1f}%)")
        if trans_limit['warning']:
            print(f"   âš ï¸  ê²½ê³ : ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}% ë„ë‹¬!")
        elif trans_limit['percentage'] > 50:
            print(f"   âš ï¸  ì£¼ì˜: ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}%")
        else:
            print(f"   âœ… ì •ìƒ: ì›”ê°„ ë¦¬ë°‹ì˜ {trans_limit['percentage']:.1f}%")

        print("\nğŸ’¡ ì°¸ê³ :")
        print("   - ì±„íŒ…/ì¸ë¼ì¸ ì œì•ˆ: AWSê°€ ê³µì‹ ë¦¬ë°‹ì„ ê³µê°œí•˜ì§€ ì•ŠìŒ (ì¶”ì  ë¶ˆê°€)")
        print("   - ì‹¤ì œ ë¦¬ë°‹ ë„ë‹¬ ì‹œ AWS ì½˜ì†”ì—ì„œ 'Monthly limit reached' ë©”ì‹œì§€ í‘œì‹œ")
        print("="*80 + "\n")

    # ì¶”ì„¸ ë¶„ì„ ì¶œë ¥
    if trends:
        print("\n" + "="*80)
        print("ğŸ“ˆ ì‚¬ìš© íŒ¨í„´ ë¶„ì„".center(80))
        print("="*80)
        print(f"  ì¼ì¼ í‰ê·  í™œë™:  {trends['daily_avg']:>10.1f}ê±´")
        print(f"  ìµœëŒ€ í™œë™ì¼:     {trends['daily_max']:>10.0f}ê±´")
        print(f"  ìµœì†Œ í™œë™ì¼:     {trends['daily_min']:>10.0f}ê±´")

        if trends["anomaly_detected"]:
            print(f"\n  ğŸš¨ ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ê°ì§€: {trends['anomaly_count']}ì¼ ë™ì•ˆ ì¼í‰ê· ({trends['daily_avg']:.1f})ì˜")
            print(f"     3ë°°({trends['anomaly_threshold']:.1f})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
            print(f"     ë¦¬ë°‹ ë„ë‹¬ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤!")
        else:
            print(f"\n  âœ… ì •ìƒ íŒ¨í„´: ì´ìƒ í™œë™ ì—†ìŒ")

        print("="*80 + "\n")


def print_s3_log_summary(stats: Dict):
    """S3 ë¡œê·¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š Amazon Q Developer S3 ë¡œê·¸ ë¶„ì„ ê²°ê³¼".center(80))
    print("="*80)

    # ê¸°ë³¸ í†µê³„
    print("\nğŸ“‹ ê¸°ë³¸ í†µê³„:")
    print(f"  ë¶„ì„ ê¸°ê°„:        {stats['period']['days']}ì¼")
    print(f"  ì´ ë¡œê·¸ íŒŒì¼:     {stats['total_log_files']:>15,}")
    print(f"  ì´ ìš”ì²­ ìˆ˜:       {stats['total_requests']:>15,}")
    print(f"  Chat ìš”ì²­:        {stats['by_type']['chat']['count']:>15,} ({stats['by_type']['chat']['count']/stats['total_requests']*100 if stats['total_requests'] > 0 else 0:.1f}%)")
    print(f"  Inline ì œì•ˆ:      {stats['by_type']['inline']['count']:>15,} ({stats['by_type']['inline']['count']/stats['total_requests']*100 if stats['total_requests'] > 0 else 0:.1f}%)")

    # í† í° ì‚¬ìš©ëŸ‰
    print("\nğŸ”¢ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰:")
    print(f"  Input í† í°:       {stats['total_input_tokens']:>15,}")
    print(f"  Output í† í°:      {stats['total_output_tokens']:>15,}")
    print(f"  ì´ í† í°:          {stats['total_tokens']:>15,}")

    # Context Window ë¶„ì„
    context_window = 200000
    usage_rate = (stats['total_tokens'] / context_window) * 100
    days_in_period = stats['period']['days']
    daily_avg = stats['total_tokens'] / days_in_period if days_in_period > 0 else 0
    daily_usage_rate = (daily_avg / context_window) * 100

    print("\nğŸ“ˆ Context Window ë¶„ì„:")
    print(f"  Context Window:   {context_window:>15,} í† í°/ì„¸ì…˜")
    print(f"  ëˆ„ì  ì‚¬ìš©ë¥ :      {usage_rate:>14.2f}% ({stats['total_tokens']:,} í† í°)")
    print(f"  ì¼ì¼ í‰ê·  í† í°:   {daily_avg:>15,.0f}")
    print(f"  ì¼ì¼ ì‚¬ìš©ë¥ :      {daily_usage_rate:>14.2f}%")
    print("\n  ğŸ’¡ Context WindowëŠ” ì„¸ì…˜ë³„ë¡œ ë…ë¦½ ê´€ë¦¬ë˜ë¯€ë¡œ,")
    print("     ëˆ„ì  ì‚¬ìš©ë¥ ë³´ë‹¤ ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.")

    # íƒ€ì…ë³„ ìƒì„¸ ë¶„ì„
    print("\nğŸ“Š íƒ€ì…ë³„ ìƒì„¸ ë¶„ì„:")

    # Chat
    chat_stats = stats['by_type']['chat']
    if chat_stats['count'] > 0:
        chat_avg_input = chat_stats['input_tokens'] / chat_stats['count']
        chat_avg_output = chat_stats['output_tokens'] / chat_stats['count']
        chat_avg_total = (chat_stats['input_tokens'] + chat_stats['output_tokens']) / chat_stats['count']

        print(f"\n  ğŸ’¬ Chat (ëŒ€í™”):")
        print(f"     ìš”ì²­ ìˆ˜:       {chat_stats['count']:>10,}")
        print(f"     í‰ê·  ì…ë ¥:     {chat_avg_input:>10,.0f} í† í°")
        print(f"     í‰ê·  ì¶œë ¥:     {chat_avg_output:>10,.0f} í† í°")
        print(f"     í‰ê·  ì´í•©:     {chat_avg_total:>10,.0f} í† í°")

    # Inline
    inline_stats = stats['by_type']['inline']
    if inline_stats['count'] > 0:
        inline_avg_input = inline_stats['input_tokens'] / inline_stats['count']

        print(f"\n  âš¡ Inline ì œì•ˆ:")
        print(f"     ìš”ì²­ ìˆ˜:       {inline_stats['count']:>10,}")
        print(f"     í‰ê·  ì»¨í…ìŠ¤íŠ¸: {inline_avg_input:>10,.0f} í† í°")
        if inline_stats['output_tokens'] == 0:
            print(f"     í‰ê·  ì¶œë ¥:     ë¡œê·¸ì— ì—†ìŒ")
        else:
            inline_avg_output = inline_stats['output_tokens'] / inline_stats['count']
            print(f"     í‰ê·  ì¶œë ¥:     {inline_avg_output:>10,.0f} í† í°")

    # ê°€ìƒ ë¹„ìš© ê³„ì‚°
    print("\nğŸ’° ê°€ìƒ ë¹„ìš© ë¶„ì„ (ì°¸ê³ ìš©):")
    print("   ğŸ’¡ Amazon Q Developer ProëŠ” $19/ì›” ì •ì•¡ì œì…ë‹ˆë‹¤.")
    print("      ì•„ë˜ ë¹„ìš©ì€ Claude APIë¥¼ ì§ì ‘ ì‚¬ìš©í–ˆì„ ê²½ìš° ê°€ì •ì…ë‹ˆë‹¤.\n")

    MODEL_PRICING = {
        "input": 0.003 / 1000,
        "output": 0.015 / 1000,
    }

    virtual_cost = (
        stats['total_input_tokens'] * MODEL_PRICING['input'] +
        stats['total_output_tokens'] * MODEL_PRICING['output']
    )

    print(f"  Input ë¹„ìš©:       ${stats['total_input_tokens'] * MODEL_PRICING['input']:>14.2f}")
    print(f"  Output ë¹„ìš©:      ${stats['total_output_tokens'] * MODEL_PRICING['output']:>14.2f}")
    print(f"  ì´ ê°€ìƒ ë¹„ìš©:     ${virtual_cost:>14.2f}")

    # ROI ë¹„êµ
    subscription_cost = 19.0
    prorated_subscription = subscription_cost * (days_in_period / 30)

    print(f"\n  êµ¬ë…ë£Œ (ì¼í• ):    ${prorated_subscription:>14.2f}")
    print(f"  ê°€ìƒ ì‚¬ìš© ë¹„ìš©:   ${virtual_cost:>14.2f}")

    savings = virtual_cost - prorated_subscription
    if savings > 0:
        savings_pct = (savings / virtual_cost) * 100
        print(f"  ì ˆê°ì•¡:           ${savings:>14.2f} ({savings_pct:.1f}% ì ˆê°)")
    else:
        loss_pct = (-savings / prorated_subscription) * 100 if prorated_subscription > 0 else 0
        print(f"  ì†ì‹¤:             ${-savings:>14.2f} ({loss_pct:.1f}% ì†ì‹¤)")

    # ì‚¬ìš©ì ì •ë³´
    if stats['by_user']:
        print(f"\nğŸ‘¥ ì‚¬ìš©ì ë¶„ì„:")
        print(f"  ë¶„ì„ëœ ì‚¬ìš©ì:    {len(stats['by_user']):>15,}ëª…")

        # ìƒìœ„ 3ëª… í‘œì‹œ
        sorted_users = sorted(
            stats['by_user'].items(),
            key=lambda x: x[1]['input_tokens'] + x[1]['output_tokens'],
            reverse=True
        )

        if len(sorted_users) > 0:
            print(f"\n  ìƒìœ„ ì‚¬ìš©ì (í† í° ê¸°ì¤€):")
            for i, (user_id, user_stats) in enumerate(sorted_users[:3], 1):
                total_tokens = user_stats['input_tokens'] + user_stats['output_tokens']
                print(f"    {i}. {user_id[:40]}...")
                print(f"       ìš”ì²­: {user_stats['requests']:,}, í† í°: {total_tokens:,}")

    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description='AWS Analytics CLI - Athena ê¸°ë°˜ (Bedrock & Amazon Q CLI)')
    parser.add_argument('--service',
                       choices=['bedrock', 'qcli'],
                       default='bedrock',
                       help='ë¶„ì„í•  ì„œë¹„ìŠ¤ (ê¸°ë³¸ê°’: bedrock)')
    parser.add_argument('--days', type=int, default=7, help='ë¶„ì„í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)')
    parser.add_argument('--region', default='us-east-1',
                       choices=list(REGIONS.keys()),
                       help='AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)')
    parser.add_argument('--start-date', help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--analysis',
                       choices=['all', 'summary', 'user', 'user-app', 'model', 'daily', 'hourly', 'feature'],
                       default='all',
                       help='ë¶„ì„ ìœ í˜• (ê¸°ë³¸ê°’: all)')
    parser.add_argument('--format',
                       choices=['terminal', 'csv', 'json'],
                       default='terminal',
                       help='ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: terminal)')
    parser.add_argument('--max-rows', type=int, default=20,
                       help='í…Œì´ë¸” ìµœëŒ€ í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--arn-pattern', type=str, default='',
                       help='ARN íŒ¨í„´ í•„í„° (Bedrockìš©, ì˜ˆ: AmazonQ-CLI, q-cli)')
    parser.add_argument('--user-pattern', type=str, default='',
                       help='ì‚¬ìš©ì ID íŒ¨í„´ í•„í„° (QCliìš©, ì˜ˆ: user@example.com)')
    parser.add_argument('--data-source',
                       choices=['s3', 'athena'],
                       default='s3',
                       help='QCli ë°ì´í„° ì†ŒìŠ¤ (s3: ì‹¤ì œ í† í°, athena: ì¶”ì •, ê¸°ë³¸ê°’: s3)')

    args = parser.parse_args()

    if args.service == 'bedrock':
        print("ğŸš€ Bedrock Analytics CLI (Athena ê¸°ë°˜)")
    else:
        data_source_desc = "S3 ë¡œê·¸ (ì‹¤ì œ í† í°)" if args.data_source == 's3' else "Athena CSV (ì¶”ì •)"
        print(f"ğŸš€ Amazon Q CLI Analytics ({data_source_desc})")
    print("="*80)

    # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    if args.start_date and args.end_date:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d')
    else:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)

    print(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    print(f"ğŸŒ ë¦¬ì „: {args.region} ({REGIONS[args.region]})")
    print(f"ğŸ“‹ ë¶„ì„ ìœ í˜•: {args.analysis}")
    print(f"ğŸ“„ ì¶œë ¥ í˜•ì‹: {args.format}")

    # ì„œë¹„ìŠ¤ë³„ í•„í„° ì„¤ì •
    if args.service == 'bedrock':
        arn_pattern = args.arn_pattern if args.arn_pattern else None
        if arn_pattern:
            print(f"ğŸ” ARN íŒ¨í„´ í•„í„°: '{arn_pattern}'")
    else:
        user_pattern = args.user_pattern if args.user_pattern else None
        if user_pattern:
            print(f"ğŸ” ì‚¬ìš©ì ID íŒ¨í„´ í•„í„°: '{user_pattern}'")
    print()

    # ì„œë¹„ìŠ¤ë³„ ë¶„ê¸° ì²˜ë¦¬
    if args.service == 'bedrock':
        # Bedrock ë¶„ì„
        analyze_bedrock(args, start_date, end_date, arn_pattern if args.arn_pattern else None)
    else:
        # QCli ë¶„ì„
        analyze_qcli(args, start_date, end_date, user_pattern if args.user_pattern else None)

    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
    logger.info("Analysis completed successfully")


def analyze_bedrock(args, start_date: datetime, end_date: datetime, arn_pattern: str = None):
    """Bedrock ë¶„ì„ ì‹¤í–‰"""
    # Tracker ì´ˆê¸°í™”
    tracker = BedrockAthenaTracker(region=args.region)

    # ë¡œê¹… ì„¤ì • í™•ì¸
    print("ğŸ” Model Invocation Logging ì„¤ì • í™•ì¸ ì¤‘...")
    current_config = tracker.get_current_logging_config()

    if current_config['status'] == 'enabled':
        print("âœ… Model Invocation Loggingì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        print(f"   S3 ë²„í‚·: {current_config['bucket']}")
        print(f"   í”„ë¦¬í”½ìŠ¤: {current_config['prefix']}")
    elif current_config['status'] == 'disabled':
        print("âŒ Model Invocation Loggingì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¨¼ì € ì„¤ì •ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”:")
        print("   python setup_bedrock_logging.py")
        return
    else:
        print(f"âš ï¸ ì„¤ì • í™•ì¸ ì¤‘ ì˜¤ë¥˜: {current_config.get('error', 'Unknown error')}")
        return

    print()
    print("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘...\n")

    # ë°ì´í„° ìˆ˜ì§‘
    results = {}

    if args.analysis in ['all', 'summary']:
        summary = tracker.get_total_summary(start_date, end_date, arn_pattern)
        results['summary'] = summary

    if args.analysis in ['all', 'user']:
        user_df = tracker.get_user_cost_analysis(start_date, end_date, arn_pattern)
        if not user_df.empty:
            # ìˆ«ì ë³€í™˜ ë° ë¹„ìš© ê³„ì‚°
            for col in ['call_count', 'total_input_tokens', 'total_output_tokens']:
                if col in user_df.columns:
                    user_df[col] = pd.to_numeric(user_df[col], errors='coerce').fillna(0)
            # ë¦¬ì „ë³„ ê°€ê²©ìœ¼ë¡œ ë¹„ìš© ê³„ì‚° (ì‚¬ìš©ìë³„ ë¶„ì„ì€ ëª¨ë¸ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ Haiku ê°€ê²© ì‚¬ìš©)
            costs = []
            for _, row in user_df.iterrows():
                input_tokens = int(row.get('total_input_tokens', 0)) if row.get('total_input_tokens') else 0
                output_tokens = int(row.get('total_output_tokens', 0)) if row.get('total_output_tokens') else 0
                # Claude 3 Haikuë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©
                cost = get_model_cost('claude-3-haiku-20240307', input_tokens, output_tokens, args.region)
                costs.append(cost)
            user_df['estimated_cost_usd'] = costs
        results['user'] = user_df

    if args.analysis in ['all', 'user-app']:
        user_app_df = tracker.get_user_app_detail_analysis(start_date, end_date, arn_pattern)
        if not user_app_df.empty:
            for col in ['call_count', 'total_input_tokens', 'total_output_tokens']:
                if col in user_app_df.columns:
                    user_app_df[col] = pd.to_numeric(user_app_df[col], errors='coerce').fillna(0)
            user_app_df = calculate_cost_for_dataframe(user_app_df, region=args.region)
        results['user_app'] = user_app_df

    if args.analysis in ['all', 'model']:
        model_df = tracker.get_model_usage_stats(start_date, end_date, arn_pattern)
        if not model_df.empty:
            for col in ['call_count', 'avg_input_tokens', 'avg_output_tokens',
                       'total_input_tokens', 'total_output_tokens']:
                if col in model_df.columns:
                    model_df[col] = pd.to_numeric(model_df[col], errors='coerce').fillna(0)
            model_df = calculate_cost_for_dataframe(model_df, region=args.region)
            # ì´ ë¹„ìš© ì—…ë°ì´íŠ¸
            if 'summary' in results:
                results['summary']['total_cost_usd'] = model_df['estimated_cost_usd'].sum()
        results['model'] = model_df

    if args.analysis in ['all', 'daily']:
        daily_df = tracker.get_daily_usage_pattern(start_date, end_date, arn_pattern)
        if not daily_df.empty:
            for col in ['call_count', 'total_input_tokens', 'total_output_tokens']:
                if col in daily_df.columns:
                    daily_df[col] = pd.to_numeric(daily_df[col], errors='coerce').fillna(0)
        results['daily'] = daily_df

    if args.analysis in ['all', 'hourly']:
        hourly_df = tracker.get_hourly_usage_pattern(start_date, end_date, arn_pattern)
        if not hourly_df.empty:
            for col in ['call_count', 'total_input_tokens', 'total_output_tokens']:
                if col in hourly_df.columns:
                    hourly_df[col] = pd.to_numeric(hourly_df[col], errors='coerce').fillna(0)
        results['hourly'] = hourly_df

    # ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ê²°ê³¼ ì¶œë ¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    if args.format == 'terminal':
        # í„°ë¯¸ë„ ì¶œë ¥
        if 'summary' in results:
            print_summary(results['summary'])

        if 'user' in results and not results['user'].empty:
            print_dataframe_table(results['user'], "ğŸ‘¥ ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¶„ì„", args.max_rows)

        if 'user_app' in results and not results['user_app'].empty:
            print_dataframe_table(results['user_app'], "ğŸ“± ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„", args.max_rows)

        if 'model' in results and not results['model'].empty:
            print_dataframe_table(results['model'], "ğŸ¤– ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„", args.max_rows)

        if 'daily' in results and not results['daily'].empty:
            print_dataframe_table(results['daily'], "ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´", args.max_rows)

        if 'hourly' in results and not results['hourly'].empty:
            print_dataframe_table(results['hourly'], "â° ì‹œê°„ë³„ ì‚¬ìš© íŒ¨í„´", args.max_rows)

    elif args.format == 'csv':
        # CSV ì €ì¥
        for key, data in results.items():
            if key == 'summary':
                continue
            if isinstance(data, pd.DataFrame) and not data.empty:
                filename = f"bedrock_{key}_{args.region}_{timestamp}.csv"
                save_to_csv(data, filename)

    elif args.format == 'json':
        # JSON ì €ì¥
        json_data = {}

        if 'summary' in results:
            json_data['summary'] = results['summary']

        for key, data in results.items():
            if key == 'summary':
                continue
            if isinstance(data, pd.DataFrame) and not data.empty:
                json_data[key] = data.to_dict(orient='records')

        filename = f"bedrock_analysis_{args.region}_{timestamp}.json"
        save_to_json(json_data, filename)


def analyze_qcli(args, start_date: datetime, end_date: datetime, user_pattern: str = None):
    """QCli ë¶„ì„ ì‹¤í–‰"""
    print("ğŸ“Š Amazon Q CLI ë°ì´í„° ë¶„ì„ ì¤‘...\n")

    # ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ë¶„ì„ ì‹¤í–‰
    if args.data_source == 's3':
        # S3 ë¡œê·¸ ë¶„ì„
        try:
            s3_analyzer = QCliS3LogAnalyzer(region=args.region, logger=logger)

            # S3 ë¡œê·¸ ë¶„ì„ ì‹¤í–‰
            stats = s3_analyzer.analyze_usage(start_date, end_date, user_pattern)

            # ê²°ê³¼ ì¶œë ¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            if args.format == 'terminal':
                print_s3_log_summary(stats)
            elif args.format == 'json':
                filename = f"qcli_s3_analysis_{args.region}_{timestamp}.json"
                save_to_json(stats, filename)
            elif args.format == 'csv':
                # S3 ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
                if stats['by_user']:
                    user_data = []
                    for user_id, user_stats in stats['by_user'].items():
                        user_data.append({
                            'ì‚¬ìš©ì ID': user_id,
                            'ìš”ì²­ ìˆ˜': user_stats['requests'],
                            'Input í† í°': user_stats['input_tokens'],
                            'Output í† í°': user_stats['output_tokens'],
                            'ì´ í† í°': user_stats['input_tokens'] + user_stats['output_tokens']
                        })
                    user_df = pd.DataFrame(user_data)
                    filename = f"qcli_s3_users_{args.region}_{timestamp}.csv"
                    save_to_csv(user_df, filename)

                if stats['by_date']:
                    date_data = []
                    for date_str, date_stats in stats['by_date'].items():
                        date_data.append({
                            'ë‚ ì§œ': date_str,
                            'ìš”ì²­ ìˆ˜': date_stats['requests'],
                            'Input í† í°': date_stats['input_tokens'],
                            'Output í† í°': date_stats['output_tokens'],
                            'ì´ í† í°': date_stats['input_tokens'] + date_stats['output_tokens']
                        })
                    date_df = pd.DataFrame(date_data)
                    filename = f"qcli_s3_daily_{args.region}_{timestamp}.csv"
                    save_to_csv(date_df, filename)

        except Exception as e:
            logger.error(f"S3 ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            print(f"âŒ S3 ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            print("ğŸ’¡ í”„ë¡¬í”„íŠ¸ ë¡œê¹…ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€, S3 ë²„í‚·ì— ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return

    else:
        # ê¸°ì¡´ Athena CSV ë¶„ì„
        tracker = QCliAthenaTracker(region=args.region)

        # ë°ì´í„° ìˆ˜ì§‘
        results = {}

        if args.analysis in ['all', 'summary']:
            summary = tracker.get_total_summary(start_date, end_date, user_pattern)
            results['summary'] = summary

            # í† í° ì¶”ì •
            token_conservative = tracker.estimate_tokens(summary, "conservative")
            token_average = tracker.estimate_tokens(summary, "average")
            token_optimistic = tracker.estimate_tokens(summary, "optimistic")

            results['token_estimates'] = {
                "conservative": token_conservative,
                "average": token_average,
                "optimistic": token_optimistic
            }

            # ë¦¬ë°‹ ì²´í¬ ë° ì¶”ì„¸ ë¶„ì„ ì¶”ê°€
            days_in_period = (end_date - start_date).days + 1
            results['limit_check'] = tracker.check_official_limits(summary, days_in_period)
            results['trends'] = tracker.analyze_usage_trends(start_date, end_date, user_pattern)

        if args.analysis in ['all', 'user']:
            user_df = tracker.get_user_usage_analysis(start_date, end_date, user_pattern)
            if not user_df.empty:
                numeric_columns = [
                    "total_chat_messages",
                    "total_inline_suggestions",
                    "total_inline_acceptances",
                    "total_chat_code_lines",
                    "total_inline_code_lines",
                    "total_dev_events",
                    "total_test_events",
                    "total_doc_events",
                    "active_days",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(user_df[col], errors='coerce').fillna(0)
            results['user'] = user_df

        if args.analysis in ['all', 'feature']:
            feature_df = tracker.get_feature_usage_stats(start_date, end_date, user_pattern)
            if not feature_df.empty:
                for col in ["total_count", "unique_users"]:
                    if col in feature_df.columns:
                        feature_df[col] = pd.to_numeric(feature_df[col], errors='coerce').fillna(0)
            results['feature'] = feature_df

        if args.analysis in ['all', 'daily']:
            daily_df = tracker.get_daily_usage_pattern(start_date, end_date, user_pattern)
            if not daily_df.empty:
                numeric_columns = [
                    "total_chat_messages",
                    "total_inline_suggestions",
                    "total_inline_acceptances",
                    "total_chat_code_lines",
                    "total_inline_code_lines",
                    "unique_users",
                ]
                for col in numeric_columns:
                    if col in daily_df.columns:
                        daily_df[col] = pd.to_numeric(daily_df[col], errors='coerce').fillna(0)
            results['daily'] = daily_df

        # ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ê²°ê³¼ ì¶œë ¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if args.format == 'terminal':
            # í„°ë¯¸ë„ ì¶œë ¥
            if 'summary' in results and 'token_estimates' in results:
                print_qcli_summary(
                    results['summary'],
                    results['token_estimates'],
                    results.get('limit_check'),
                    results.get('trends')
                )

            if 'user' in results and not results['user'].empty:
                print_dataframe_table(results['user'], "ğŸ‘¥ ì‚¬ìš©ìë³„ ë¶„ì„", args.max_rows)

            if 'feature' in results and not results['feature'].empty:
                print_dataframe_table(results['feature'], "ğŸ“± ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„", args.max_rows)

            if 'daily' in results and not results['daily'].empty:
                print_dataframe_table(results['daily'], "ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´", args.max_rows)

        elif args.format == 'csv':
            # CSV ì €ì¥
            for key, data in results.items():
                if key in ['summary', 'token_estimates']:
                    continue
                if isinstance(data, pd.DataFrame) and not data.empty:
                    filename = f"qcli_{key}_{args.region}_{timestamp}.csv"
                    save_to_csv(data, filename)

        elif args.format == 'json':
            # JSON ì €ì¥
            json_data = {}

            if 'summary' in results:
                json_data['summary'] = results['summary']

            if 'token_estimates' in results:
                json_data['token_estimates'] = results['token_estimates']

            for key, data in results.items():
                if key in ['summary', 'token_estimates']:
                    continue
                if isinstance(data, pd.DataFrame) and not data.empty:
                    json_data[key] = data.to_dict(orient='records')

            filename = f"qcli_analysis_{args.region}_{timestamp}.json"
            save_to_json(json_data, filename)


if __name__ == "__main__":
    main()
